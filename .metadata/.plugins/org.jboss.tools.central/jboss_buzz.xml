<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Custom logic in BPMN</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XF6UrQdTZ0M/custom-logic-in-bpmn.html" /><author><name>Kirill Gaevskii</name></author><id>https://blog.kie.org/2021/06/custom-logic-in-bpmn.html</id><updated>2021-06-24T08:04:08Z</updated><content type="html">There are several ways to add additional custom logic to the Business Process. This article provides a review of different possibilities and their pros and cons. EMBEDDED CODE The easiest way, but not the most comfortable, is to use Script Task and its Script field. When you work with a Script Task, all you need to do is add the node to the process, and you are ready to write Java/JavaScript/MVEL code using the Script property of the task: &gt; NOTE: JavaScript is deprecated and can be removed in the following releases This way of adding custom logic to the process is very fast. Also, there are no performance issues since all scripts are processed just once with Java code and other assets during the project compile phase. However, this way is not very convenient in the long term due to hard maintainability. WHY USE SCRIPT FIELDS * No configuration is needed, just write your code! * As fast as any Java logic * During process model time, there is no need to understand implementation differences between task types DOWNSIDES OF SCRIPT FIELDS * It is not possible to debug this code * Code block has limited syntax highlight and autocompletes features * It’s hard to track changes using version control systems * The use of fully qualified names (FQNs) to use classes. As an alternative import the class for the whole process * Code can be removed unintentionally by morphing Script Task to any other task type &gt; NOTE: for jBPM there is also an additional option to write quick snippets of &gt; code with the same benefits and downsides as for Script fields: On Entry and &gt; On Exit Actions for different types of activities. This functionality is not &gt; present today in Kogito but it will be available very soon CUSTOM TASK Custom Tasks is a powerful feature. You can predefine the node’s different visual and runtime properties on the canvas. An example can be predefined input and output parameters, as well as a custom task icon, task name, documentation, and other task parameters. Also, using Custom Task, you can specify the custom of the task, which can be used to create a unique experience for the new BPMN node. Custom Tasks explained in detail in another article. Below are the pros and cons of this type of activity: WHY USE CUSTOM TASK * It’s possible to debug Custom Task’s logic * All code will be edited directly in VS Code with all perks like syntax highlight and autocomplete * It’s possible to write unit tests for the Task’s logic * It’s easy to track changes using version control systems * It’s possible to predefine different parameters such as Data Input/Outputs * It is possible to predefine some visual parameters as well, like icon and task name * Custom Task can be used as a good template to reuse similar tasks parameters for the whole project DOWNSIDES OF CUSTOM TASK * You need to use a specific task structure and interface, which makes logic a bit harder to reuse between different task types * During the process model, the user should orient in implementation details. Instead of the small number of well-defined tasks (Manual, Automated, Rules) user will see a big amount of different tasks * If you are using different flavors of BPMN Designer like VS Code, browser plugin, standalone editor, desktop version, and so on, you need to take care to support the WID file between all those editor’s versions. * It is not possible to simply share BPMN files between two and more people. To do so, WID files should be shared and correctly used as well. * All task properties configurations  and configuration files should be located correctly. SERVICE TASK Service Task is a compromise between simplicity of Script Task and features of Custom Task and usually suitable for most of the use cases. Service task usage doesn’t need additional knowledge about implementation for Business Analysts, no need for additional files. Also not tied to any interface and don’t need any registration as Custom tasks do. &gt; NOTE: In jBPM, Service Tasks can’t be placed inside of normal Java application &gt; life cycle which means it is not possible to use features like Java &gt; Annotations or CDI in Service Task if the project will run in jBPM. That’s why &gt; Custom Tasks can be preferable for jBPM projects. However, if the project will &gt; run under Kogito all features from the Java life cycle are available and &gt; Service Task can be a preferable choice. HOW TO USE SERVICE TASK ON KOGITO To use the Service task, you will need a Java bean located in your project or project dependencies and mark it with CDI annotation. package com.github.hasys; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class TestService { public String greetUser(String name, Integer age) { System.out.println(String.format("Hello %s with age %o", name, age)); return String.format("User %s greeted.", name); } } To configure Service Task to use this Java bean, its FQN should be used as Interface property and method name as Operation. Assignments are used to send data to and get data from the Service Task. The output will always have the name Result for jBPM and can be any for Kogito. Inputs should be the same as the method’s parameters for both jBPM and Kogito: You can check examples in detail in our repository. WHY USE SERVICE TASK * It’s possible to debug Custom Task’s logic * All code will be edited directly in VS Code with all perks like syntax highlight and autocomplete * It’s possible to write unit tests for the Task’s logic * It’s easy to track changes using version control systems * The process can be shared with Business Analysts without additional files and configurations in different flavors of BPMN Editor * During process model time, there is no need to understand implementation differences between tasks * For service tasks, plain Java bean classes used, which makes business logic flexible and reusable DOWNSIDES OF SERVICE TASK * Not possible to predefine different Task parameters * Java bean should be linked for each Service Task separately CONCLUSION There are several possibilities for how to add custom logic to your Process. All of them have their benefits and limitations to fill needs for any use cases. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XF6UrQdTZ0M" height="1" width="1" alt=""/&gt;</content><dc:creator>Kirill Gaevskii</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/custom-logic-in-bpmn.html</feedburner:origLink></entry><entry><title>Automating rule-based services with Java and Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gO2utZKK7QQ/automating-rule-based-services-java-and-kogito" /><author><name>Karina Varela</name></author><id>7ca37207-8e6d-495d-ada6-d782a14638cd</id><updated>2021-06-24T07:00:00Z</updated><published>2021-06-24T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/topics/automation/whats-business-automation"&gt;Business automation&lt;/a&gt; today is a constant and critical task for organizations that seek to formalize policies and ensure that they can be executed, maintained, monitored, and applied during daily operations. This article demonstrates how to use the Kogito engine to automate business rules by implementing them in the Drools Rules Language (DRL).&lt;/p&gt; &lt;p&gt;DRL is common in Drools-based projects. To start using it with Kogito, you need to understand the concept of &lt;em&gt;rule units&lt;/em&gt;. You'll learn how to work with rule units in practice by writing a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; service that automates a piece of business logic with rule units and minimal coding. These capabilities are now part of &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt; 7.11.x, released on June 17.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito"&gt;&lt;em&gt;How to deliver decision services with Kogito&lt;/em&gt;&lt;/a&gt; for more about creating a decision service with Decision Model and Notation (DMN) in Kogito and Quarkus.&lt;/p&gt; &lt;h2&gt;Automating business rules with Kogito&lt;/h2&gt; &lt;p&gt;Bringing all the maturity of Drools, Kogito is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project with a lightweight, fast, and cloud-native runtime engine. Developers using Kogito can expect runtime environment improvements and new a tooling set to use along with IDEs like VS Code. Starting with Process Automation Manager version 7.11.x, it is possible to use a supported version of Kogito based on version 1.5.x . You can choose between running decision services with two runtimes: The &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Red Hat build of Quarkus&lt;/a&gt; 1.11.x or &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; 2.3.4.&lt;/p&gt; &lt;p&gt;In Kogito, you can define business rules using Drools Rules Language or decision tables written in the XLS format. If you are familiar with decision tables, you shouldn't notice many differences when using them with Kogito. To use DRL-based rules, however, you need to get used to the concept of rule units.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;rule unit&lt;/em&gt; aggregates a set of rules along with a description of the working memory that these rules will act upon, also called &lt;em&gt;data sources&lt;/em&gt;. Kogito uses rule units to generate an executable model based on the rules, with a faster startup time. Kogito Codegen also takes advantage of Drools Rules Language queries to discover possible &lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt; to expose via REST, based on queries defined in the rule unit.&lt;/p&gt; &lt;h2&gt;Using rule units in Kogito&lt;/h2&gt; &lt;p&gt;We'll get started with Kogito and rule units through a simple example that defines a person and whether that person is an adult.&lt;/p&gt; &lt;p&gt;Start with a POJO (plain old Java object) named &lt;code&gt;Person&lt;/code&gt; with the following attributes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme.domain; public class Person { private String name; private int age; private boolean adult; // (getters and setters omitted) } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we can define the rule unit, which we call &lt;code&gt;PersonUnit&lt;/code&gt;. It extends the &lt;code&gt;RuleUnitData&lt;/code&gt; class &lt;code&gt;org.kie.kogito.rules&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; package org.acme; import org.acme.domain.Person; import org.kie.kogito.rules.DataSource; import org.kie.kogito.rules.DataStore; import org.kie.kogito.rules.RuleUnitData; public class PersonUnit implements RuleUnitData { private DataStore&lt;Person&gt; persons = DataSource.createStore(); private int adultAge; public PersonUnit() { } //getters and setters omitted } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Our DRL rule implementation is linked to the rule unit we have just defined by the declaration unit &lt;code&gt;PersonUnit&lt;/code&gt;. To write the &lt;code&gt;Is Adult&lt;/code&gt; rule, we use OOPath, a version of the standard &lt;a href="https://www.w3.org/TR/xpath/"&gt;XPath&lt;/a&gt; language that is designed for use with objects. It provides a succinct and readable way to work with DRL.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; package org.acme; unit PersonUnit; //This links this rule back to our RuleUnit import org.acme.domain.Person; rule "Is Adult" when $p: /persons[age &gt;= 18]; //Tests the age of any Person object in the datastore persons declared in the rule unit then $p.setAdult(true); end query "adult" // Allows Kogito code-gen to expose a domain-driven api to search for adults. $p: /persons; end &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;As this example shows, Kogito helps business automation developers create lightweight rules services quickly with Quarkus or Spring Boot, package the services traditionally or using native compilation, and even deploy them as functions with &lt;a href="https://knative.dev/"&gt;KNative&lt;/a&gt; on top of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. For resource planning, Process Automation Manager fully supports &lt;a href="https://www.optaplanner.org/"&gt;OptaPlanner&lt;/a&gt; version 8, the most recent version of this &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; constraint-solver technology.&lt;/p&gt; &lt;h2&gt;Related tools and support&lt;/h2&gt; &lt;p&gt;If you are interested in trying out Kogito and rule units, check out these places where you can get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The OpenShift interactive &lt;a href="https://learn.openshift.com/middleware/courses/middleware-kogito/rules"&gt;learning portal for Kogito rules&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-using-drl-rules"&gt;Kogito documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The community channels at the &lt;a href="http://kie.org/"&gt;Kogito community&lt;/a&gt;, including its core contributors team.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Also see &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito"&gt;my previous article&lt;/a&gt;, which has more information about using Kogito decision services with Decision Model and Notation (DMN) and creating a decision service with Kogito and Quarkus.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Kogito brings back the joy for business automation developers, who can now quickly develop lightweight rules services, package them traditionally or using native compilation, and even deploy them as functions with &lt;a href="https://developers.redhat.com/books/knative-cookbook"&gt;KNative on top of Kubernetes and OpenShift&lt;/a&gt;. Try it out today, share your feedback, and contribute. The architectural possibilities are endless and flexible to suit on-premises or cloud environments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/24/automating-rule-based-services-java-and-kogito" title="Automating rule-based services with Java and Kogito"&gt;Automating rule-based services with Java and Kogito&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gO2utZKK7QQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Karina Varela</dc:creator><dc:date>2021-06-24T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/24/automating-rule-based-services-java-and-kogito</feedburner:origLink></entry><entry><title>How the JIT compiler boosts Java performance in OpenJDK</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/s8alKH6PfXc/how-jit-compiler-boosts-java-performance-openjdk" /><author><name>Roland Westrelin</name></author><id>36a9d0ce-3c1d-4906-83f9-32df0df7a5c5</id><updated>2021-06-23T07:00:00Z</updated><published>2021-06-23T07:00:00Z</published><summary type="html">&lt;p&gt;Just-in-time (JIT) compilation is central to peak performance in modern virtual machines, but it comes with trade-offs. This article introduces you to JIT compilation in HotSpot, &lt;a href="https://developers.redhat.com/products/openjdk/"&gt;OpenJDK&lt;/a&gt;'s &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; virtual machine. After reading the article, you will have an overview of HotSpot's multi-tiered execution model and how it balances the resources required by your Java applications and by the compiler itself. You'll also see two examples that demonstrate how a JIT compiler uses advanced techniques—deoptimization and speculation—to boost application performance.&lt;/p&gt; &lt;h2&gt;How a JIT compiler works&lt;/h2&gt; &lt;p&gt;At its core, a JIT compiler relies on the same well-known compilation techniques that an offline compiler such as the GNU Compiler Collection (GCC) uses. The primary difference is that a just-in-time compiler runs in the same process as the application and competes with the application for resources. As a consequence, a different set of trade-offs in the JIT compiler design applies. Mainly, compilation time is more of an issue for a JIT compiler than for an offline compiler, but new possibilities for optimization—such as deoptimization and speculation—open up, as well.&lt;/p&gt; &lt;h2&gt;The JIT compiler in OpenJDK&lt;/h2&gt; &lt;p&gt;A Java-based JIT compiler takes &lt;code&gt;.class&lt;/code&gt; files as input rather than Java code, which is consumed by &lt;code&gt;javac&lt;/code&gt;. In this way, a JIT compiler differs from a compiler like GCC, which directly consumes the code that you produce. The JIT compiler's role is to turn class files (composed of &lt;em&gt;bytecode&lt;/em&gt;, which is the JVM's instruction set) into machine code that the CPU executes directly. Another way a Java JIT compiler differs from an offline compiler is that the JVM verifies class files at load time. When it's time to compile, there's little need for parsing or verification.&lt;/p&gt; &lt;p&gt;Other than performance variations, the JIT compiler's execution is transparent to the end user. It can, however, be observed by running the &lt;code&gt;java&lt;/code&gt; command with diagnostic options. As an example, take this simple HelloWorld program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;public class HelloWorld { public static void main(String[] args) { System.out.println("Hello world!"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's the output from running this program with the version of OpenJDK that happens to be installed on my system:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ java -XX:+PrintCompilation HelloWorld      50    1       3       java.lang.Object::&lt;init&gt; (1 bytes)      50    2       3       java.lang.String::hashCode (55 bytes)      51    3       3       java.lang.String::indexOf (70 bytes)      51    4       3       java.lang.String::charAt (29 bytes)      51    5     n 0       java.lang.System::arraycopy (native)   (static)      52    6       3       java.lang.Math::min (11 bytes)      52    7       3       java.lang.String::length (6 bytes)      52    8       3       java.lang.AbstractStringBuilder::ensureCapacityInternal (27 bytes)      52    9       1       java.lang.Object::&lt;init&gt; (1 bytes)      53    1       3       java.lang.Object::&lt;init&gt; (1 bytes)   made not entrant      55   10       3       java.lang.String::equals (81 bytes)      57   11       1       java.lang.ref.Reference::get (5 bytes)      58   12       1       java.lang.ThreadLocal::access$400 (5 bytes) Hello world!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Most lines of the diagnostic output list a method that's JIT-compiled. However, a quick scan of the output reveals that neither the &lt;code&gt;main&lt;/code&gt; method nor &lt;code&gt;println&lt;/code&gt; are compiled. Instead, a number of methods not present in the &lt;code&gt;HelloWorld&lt;/code&gt; class were compiled. This brings up a few questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Where do those methods come from, if not from HelloWorld? &lt;/strong&gt;Some code supporting the JVM startup, as well as most of the Java standard library, is implemented in Java, so it is subject to JIT compilation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If none of its methods are compiled, then how does the HelloWorld class run? &lt;/strong&gt;In addition to a JIT compiler, HotSpot embeds an interpreter, which encodes the execution of each JVM bytecode in the most generic way. All methods start executing interpreted, where each bytecode is executed one at a time. Compiled code is tailored to a particular Java method, so optimizations can be applied across bytecode boundaries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why not have the JIT compiler prepare the code for faster results? &lt;/strong&gt;That would require the user code to wait for the JIT'ed code to be ready, which would cause a noticeable pause because compilation takes time.&lt;/p&gt; &lt;h2&gt;HotSpot's JIT execution model&lt;/h2&gt; &lt;p&gt;In practice, the HotSpot JVM's execution model is the result of four observations taken together:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Most code is only executed uncommonly, so getting it compiled would waste resources that the JIT compiler needs.&lt;/li&gt; &lt;li&gt;Only a subset of methods is run frequently.&lt;/li&gt; &lt;li&gt;The interpreter is ready right away to execute any code.&lt;/li&gt; &lt;li&gt;Compiled code is much faster but producing it is resource hungry, and it is only available after the compilation process is over which takes time.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The resulting execution model could be summarized as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Code starts executing interpreted with no delay.&lt;/li&gt; &lt;li&gt;Methods that are found commonly executed (hot) are JIT compiled.&lt;/li&gt; &lt;li&gt;Once compiled code is available, the execution switches to it.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Multi-tiered execution&lt;/h3&gt; &lt;p&gt;In HotSpot, the interpreter instruments the code that it executes; that is, it maintains a per-method count of the number of times a method is entered. Because hot methods usually have loops, it also collects the number of times a branch back to the start of a loop is taken. On method entry, the interpreter adds the two numbers and if the result crosses a threshold, it enqueues the method for compilation. A compiler thread running concurrently with threads executing Java code then processes the compilation request. While compilation is in progress, interpreted execution continues, including for methods in the process of being JIT'ed. Once the compiled code is available, the interpreter branches off to it.&lt;/p&gt; &lt;p&gt;So, the trade-off is roughly between the fast-to-start-but-slow-to-execute interpreter and the slow-to-start-but-fast-to-execute compiled code. How slow-to-start that compiled code is, is under the virtual machine designer's control to some extent: The compiler can be designed to optimize less (in which case code is available sooner but doesn't perform as well) or more (leading to faster code at a later time). A practical design that leverages this observation is to have a multi-tier system.&lt;/p&gt; &lt;h3&gt;The three tiers of execution&lt;/h3&gt; &lt;p&gt;HotSpot has a three-tiered system consisting of the interpreter, the quick compiler, and the optimizing compiler. Each tier represents a different trade-off between the delay of execution and the speed of execution. Java code starts execution in the interpreter. Then, when a method becomes &lt;em&gt;warm&lt;/em&gt;, it's enqueued for compilation by the quick compiler. Execution switches to that compiled code when it's ready. If a method executing in the second tier becomes &lt;em&gt;hot&lt;/em&gt;, then it's enqueued for compilation by the optimizing compiler. Execution continues in the second-tier compiled code until the faster code is available. Code compiled at the second tier has to identify when a method becomes hot, so it also has to increment invocation and back-branch counters.&lt;/p&gt; &lt;p&gt;Note that this description is simplified: The implementation tries to not overwhelm compiler threads with requests, and to balance execution speed with compilation load. As a consequence, thresholds that trigger compilations are not fixed and the second tier is actually split into several sub-tiers.&lt;/p&gt; &lt;p&gt;In HotSpot, for historical reasons, the second tier is known as C1 or the client compiler and the optimizing tier is known as C2, or the server compiler.&lt;/p&gt; &lt;p&gt;In the HelloWorld example, the third column of numbers in the diagnostic output identifies the tier at which code is compiled. Tiers 1 to 3 are subtiers of the low-tier compiler. Tier 4 is the optimizing compiler. As can be seen in the output, that example is so short-lived that no method reaches the optimizing compiler.&lt;/p&gt; &lt;h2&gt;Deoptimization and speculation&lt;/h2&gt; &lt;p&gt;Figure 1 shows a simplified diagram of the state of a method execution.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="This simple method flow moves from left to right." data-entity-type="file" data-entity-uuid="b262fb54-e618-4d55-a387-59b3d8a78a42" src="https://developers.redhat.com/sites/default/files/inline-images/JIT%20in%20hotspot%20blog%20post1_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The state of a method execution moving from left to right.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;State changes happen left to right. This is incomplete as, perhaps surprisingly, state transitions also exist right to left (that is, from more optimized code to less optimized code), as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="State transitions move in a loop: from left to right and from right to left." data-entity-type="file" data-entity-uuid="d45a679b-7ed1-4e62-9073-3ed4408be405" src="https://developers.redhat.com/sites/default/files/inline-images/JIT%20in%20hotspot%20blog%20post2.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: State transitions also move from right to left.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In HotSpot jargon, that process is called &lt;em&gt;deoptimization&lt;/em&gt;. When a thread deoptimizes, it stops executing a compiled method at some point in the method and resumes execution in the same Java method at the exact same point, but in the interpreter.&lt;/p&gt; &lt;p&gt;Why would a thread stop executing compiled code to switch to much slower interpreted code? There are two reasons. First, it is sometimes convenient to not overcomplicate the compiler with support for some feature's uncommon corner case. Rather, when that particular corner case is encountered, the thread deoptimizes and switches to the interpreter.&lt;/p&gt; &lt;p&gt;The second and main reason is that deoptimization allows the JIT compilers to &lt;em&gt;speculate&lt;/em&gt;. When speculating, the compiler makes assumptions that should prove correct given the current state of the virtual machine, and that should let it generate better code. However, the compiler can't prove its assumptions are true. If an assumption is invalidated, then the thread that executes a method that makes the assumption deoptimizes in order to not execute code that's erroneous (being based on wrong assumptions).&lt;/p&gt; &lt;h3&gt;Example 1: Null checks in the C2 tier&lt;/h3&gt; &lt;p&gt;An example of speculation that C2 uses extensively is its handling of null checks. In Java, every field or array access is guarded by a null check. Here is an example in pseudocode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; if (object == null) {   throw new NullPointerException(); } val = object.field;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's very uncommon for a &lt;code&gt;NullPointerException&lt;/code&gt; (NPE) to not be caused by a programming error, so C2 speculates—though it cannot prove it—that NPEs never occur. Here's that speculation in pseudocode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; if (object == null) {   deoptimize(); } val = object.field;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If NPEs never occur, all the logic for exception creation, throwing, and handling is not needed. What if a null object is seen at the field access in the pseudocode? The thread deoptimizes, a record is made of the failed speculation, and the compiled method's code is dropped. On the next JIT compilation of that same method, C2 will check for a failed speculation record before speculating again that no null object is seen at the field access.&lt;/p&gt; &lt;h3&gt;Example 2: Class hierarchy analysis&lt;/h3&gt; &lt;p&gt;Now, let's look at another example, starting with the following code snippet:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;class C {   void virtualMethod() {} } void compiledMethod(C c) {   c.virtualMethod(); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The call in &lt;code&gt;compiledMethod()&lt;/code&gt; is a virtual call. With only class &lt;code&gt;C&lt;/code&gt; loaded but none of its potential subclasses, that call can only invoke &lt;code&gt;C.virtualMethod()&lt;/code&gt;. When &lt;code&gt;compiledMethod()&lt;/code&gt; is JIT compiled, the compiler could take advantage of that fact to devirtualize the call. But what if, at a later point, a subclass of class &lt;code&gt;C&lt;/code&gt; is loaded? Executing &lt;code&gt;compiledMethod()&lt;/code&gt;, which is compiled under the assumption that &lt;code&gt;C&lt;/code&gt; has no subclass, could then cause an incorrect execution.&lt;/p&gt; &lt;p&gt;The solution to that problem is for the JIT compiler to record a dependency between the compiled method of &lt;code&gt;compiledMethod()&lt;/code&gt; and the fact that &lt;code&gt;C&lt;/code&gt; has no subclass. The compiled method's code itself doesn't embed any extra runtime check and is generated as if &lt;code&gt;C&lt;/code&gt; had no subclass. When a method is loaded, dependencies are checked. If a compiled method with a conflicting dependency is found, that method is marked for deoptimization. If the method is on the call stack of a thread, when execution returns to it, it immediately deoptimizes. The compiled method for &lt;code&gt;compiledMethod()&lt;/code&gt; is also made &lt;em&gt;not entrant&lt;/em&gt;, so no thread can invoke it. It will eventually be reclaimed. A new compiled method can be generated that will take the updated class hierarchy into account. This is known as &lt;em&gt;class hierarchy analysis&lt;/em&gt; (CHA).&lt;/p&gt; &lt;h3&gt;Synchronous and asynchronous deoptimization&lt;/h3&gt; &lt;p&gt;Speculation is a technique that's used extensively in the C2 compiler beyond these two examples. The interpreter and lowest tier actually collect profile data (at subtype checks, branches, method invocation, and so on) that C2 leverages for speculation. Profile data either count the number of occurrences of an event—the number of times a method is invoked, for instance—or collect constants (that is, the type of an object seen at a subtype check).&lt;/p&gt; &lt;p&gt;From these examples, we can see two types of deoptimization events: &lt;em&gt;Synchronous events&lt;/em&gt; are requested by the thread executing compiled code, as we saw in the example of the null check. These events are also called &lt;em&gt;uncommon traps&lt;/em&gt; in HotSpot. &lt;em&gt;Asynchronous events&lt;/em&gt; are requested by another thread, as we saw in the example of the class hierarchy analysis.&lt;/p&gt; &lt;h3&gt;Safepoints and deoptimization&lt;/h3&gt; &lt;p&gt;Methods are compiled so deoptimization is only possible at locations known as &lt;em&gt;safepoints&lt;/em&gt;. Indeed, on deoptimization, the virtual machine has to be able to reconstruct the state of execution so the interpreter can resume the thread at the point in the method where compiled execution stopped. At a safepoint, a mapping exists between elements of the interpreter state (locals, locked monitors, and so on) and their location in compiled code—such as a register, stack, etc.&lt;/p&gt; &lt;p&gt;In the case of a synchronous deoptimization (or uncommon trap), a safepoint is inserted at the point of the trap and captures the state needed for the deoptimization. In the case of an asynchronous deoptimization, the thread in compiled code has to reach one of the safepoints that were compiled in the code in order to deoptimize.&lt;/p&gt; &lt;p&gt;As part of the optimization process, compilers often re-order operations so the resulting code runs faster. Re-ordering operations across a safepoint in that way would cause the state at the safepoint to differ from the state expected at that location by the interpreter and can’t be allowed. As a consequence, it’s not feasible to have a safepoint for every bytecode of a method. Doing so would constrain optimizations too much. A compiled method only includes a few safepoints (on return, at calls, and in loops) and a balance is needed between &lt;em&gt;common enough&lt;/em&gt; safepoints, so a deoptimization is not delayed, and &lt;em&gt;rare enough&lt;/em&gt; safepoints, so the compiler has the freedom to optimize between two of them.&lt;/p&gt; &lt;p&gt;This also affects garbage collection and other virtual machine operations that rely on safepoints. Indeed, garbage collection operations need the locations of live objects on a thread’s stacks, which are only available at safepoints. In general, in compiled code, safepoints are the only locations where state that the virtual machine can work with is available.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;To summarize the discussion in this article, a JIT compiler, at its core, is just another compiler. But because it shares resources with the application, the amount of resources used for compilation must be balanced with the benefits of having compiled code. Running concurrently with the application code also has a benefit: It allows compiled code to be tailored to the observed state of the virtual machine through speculation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk" title="How the JIT compiler boosts Java performance in OpenJDK"&gt;How the JIT compiler boosts Java performance in OpenJDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/s8alKH6PfXc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Roland Westrelin</dc:creator><dc:date>2021-06-23T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk</feedburner:origLink></entry><entry><title type="html">Getting started with Red Hat Business Automation version 7.11</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Tts7KfdRalc/getting-started-with-red-hat-business-automation-version-711.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/f44Ha1v5S4U/getting-started-with-red-hat-business-automation-version-711.html</id><updated>2021-06-23T05:00:00Z</updated><content type="html"> This last week the new release of the Red Hat Business Automation products went live, spanning and with a new version 7.11. These two products provide a lot over versatility to your developer toolbox and there are some getting started documentation and examples to be found in the above linked pages. Over the years I've shared so around all the generations of Red Hat Business Automation products, that you might like to have a little overview of the ones that are now fully updated for use? Below you'll find a walk through the various projects, demos, and workshops available today for you to get started with the latest and greatest of Red Hat Business Automation tools. Let's start with the updates that are designed to show you how to get started with the business automation tooling in a hands-on exploratory fashion. This is a true for building online retail web shop using Red Hat Decision Manager. It's designed to walk you from nothing, to installing the decision manager tooling, to building your project, leveraging a front end project, and deploying it all on your local machine. Click on image to open workshop and view table of contents This project is used in the above workshop to quickly get you up and running with the latest decision manager tooling. In just you're fully installed and ready to start developing your projects. An independent project that with a Vaadin front end and rules project supporting online shopping experience. Get hands-on in just minutes with this complete project. This is a true  for building a human resources employee rewards process using Red Hat Process Automation Manager. It's designed to walk you from nothing, to installing the process automation tooling, to building your project, and deploying it all on your local machine. Click on image to open workshop and view table of contents This project is used in the above workshop to quickly get you up and running with the latest process automation tooling. In just  you're fully installed and ready to start developing your projects. After all of this we have another workshop to get a bit deeper into a real life use case outside of the normal process world. This is another  for building a four eyes principle DevOps process project using Red Hat Process Automation Manager. Click on image to open workshop and view table of contents This project is with human task integration, rule integration, task forms, and features the use of a signal to add a customer contact moment for the marketing department. It installs the process automation tooling and project on your local machine in an automated fashion. Note that all of these projects and workshops are targeting your local machine, in the next article I'll share more content that can be used in containers, operators, and on the OpenShift in a cloud native fashion.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Tts7KfdRalc" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/f44Ha1v5S4U/getting-started-with-red-hat-business-automation-version-711.html</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tCiWOcx72HQ/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" /><author><name>Wojciech Trocki</name></author><id>10f9bcc0-0deb-43fc-b6ef-989de0c9520a</id><updated>2021-06-22T07:00:00Z</updated><published>2021-06-22T07:00:00Z</published><summary type="html">&lt;p&gt;In this part of our ongoing introduction to the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture&lt;/a&gt;, we dig into some of the discussions the team had when developing the GraphQL section of the reference architecture. Learn about the principles we considered and gain additional insight into how we developed the current recommendations for using GraphQL in your Node.js applications.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Part 1&lt;/a&gt;: Overview of the Node.js reference architecture&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Part 2&lt;/a&gt;: Logging in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Part 3&lt;/a&gt;: Code consistency in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Part 4&lt;/strong&gt;: GraphQL in Node.js&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;GraphQL in the Node.js ecosystem&lt;/h2&gt; &lt;p&gt;GraphQL is a &lt;a href="https://graphql.org/learn/" target="_blank"&gt;query language specification&lt;/a&gt; that includes specific semantics for interaction between the client and server. Implementing a GraphQL server and client typically requires more effort than building REST applications, due to the extensive nature of the language and additional requirements for client-side and server-side developers. To start, let's consider a few of the elements of developing a Node.js application with GraphQL (Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png?itok=EeUSKWlI" width="600" height="643" alt=""We need to build a new app. Let's use GraphQL!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Let's use GraphQL for our new app. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Developing a GraphQL schema&lt;/h3&gt; &lt;p&gt;When building a GraphQL API, client- and server-side teams must define strong contracts in the form of a GraphQL schema. The two teams must also change the way they have been communicating and developing their software. GraphQL internally requires server-side developers to build data-handling methods, called &lt;em&gt;resolvers&lt;/em&gt;, that match the &lt;em&gt;GraphQL schema&lt;/em&gt;, which is an internal graph that both teams must build and agree on. Client-side developers typically need to use specialized clients to send GraphQL queries to the back-end server.&lt;/p&gt; &lt;h3&gt;Choosing your tools&lt;/h3&gt; &lt;p&gt;The GraphQL ecosystem consists of thousands of libraries and solutions that you can find on GitHub, at conferences, and in various forums that offer to resolve all your GraphQL problems. On top of frameworks and libraries (Figure 2) the GraphQL ecosystem offers many out-of-the-box, self-hosted, or even service-based (SaaS) CRUD engines. Create, read, update, and delete (CRUD) engines offer to minimize the amount of server-side development by providing a direct link to the database. We'll come back to this topic later.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png?itok=GHYpODOu" width="600" height="660" alt=""Should we use a service, framework, or CRUD engine?"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: What tools will we use to enable GraphQL? &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Implementing a GraphQL API&lt;/h3&gt; &lt;p&gt;When implementing a GraphQL API, we often see a number of side-effects on other elements of our back-end infrastructure. A GraphQL API is typically exposed as a single endpoint by our back end, as illustrated in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-3.png?itok=S1EFBsvN" width="600" height="397" alt="The visual shows that a GraphQL endpoint is exposed as a single endpoint." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Unlike a REST API, a GraphQL API is exposed as a single endpoint. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Adopting the GraphQL API means that we will not only need to change the API but will often have to rethink our entire infrastructure (Figure 4), from API management and security to caching, developing a federation of queries on gateways, and much more.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png?itok=xcMaHNpT" width="600" height="663" alt="How do we get the product to work with the legacy back end?" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Think through your GraphQL-based application before implementing it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Schema first or code first?&lt;/h3&gt; &lt;p&gt;There are multiple ways to develop GraphQL solutions. The two most common approaches are &lt;em&gt;schema first&lt;/em&gt;, where developers write GraphQL schema first and later build client-side queries and data resolvers on the back end, and &lt;em&gt;code first&lt;/em&gt; (also known as resolvers first), where developers write the resolvers first and then generate the GraphQL schema for them.&lt;/p&gt; &lt;p&gt;Both approaches come with advantages and disadvantages based on your specific use case.&lt;/p&gt; &lt;h2&gt;Implementing GraphQL for Node.js&lt;/h2&gt; &lt;p&gt;Making all of the decisions about how to implement GraphQL can be daunting, as illustrated by Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png?itok=mKvv77y7" width="600" height="672" alt=""Doing this on my own is really going to take time."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Implementing GraphQL for Node.js is no simple task. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Many developers become overwhelmed by the amount of work required and look for libraries or tools that offer comprehensive support instead. As we've previously mentioned, in a GraphQL ecosystem, developers often look to one of the available CRUD engines for support (Figure 6).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png?itok=enT_X-xt" width="600" height="660" alt=""Let's use a CRUD engine!"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using a CRUD engine is a tempting workaround. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;CRUD engines try to address the major shortcomings and complexity of GraphQL by offering unified and low-code data access. However, in the long run, they can fail to deliver the capabilities we want, especially integration with other services.&lt;/p&gt; &lt;p&gt;Moreover, the initial results associated with using productivity tooling are often the tip of the iceberg for what you will face when deploying your code to production (see Figure 7).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-7.png?itok=L_ANtheg" width="600" height="396" alt="Tools only address the surface issues of development." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Considerations for developing a Node.js application with GraphQL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Red Hat team members have been using GraphQL for many years, working with the community and customers to address different challenges encountered when using GraphQL, including those we've discussed in the preceding sections. Next, we'll introduce the GraphQL section of the Node.js Reference architecture, which is based on our experience as well as that of teams within IBM.&lt;/p&gt; &lt;h2&gt;GraphQL recommendations and guidance&lt;/h2&gt; &lt;p&gt;When working on the GraphQL section of the reference architecture, we discussed a number of principles and values that influenced the documented recommendations and guidance. Here, we'll offer a brief overview.&lt;/p&gt; &lt;h3&gt;Schema first development&lt;/h3&gt; &lt;p&gt;In order to support collaboration across different languages, microservices, and tools we recommend using the GraphQL schema as a form of API definition rather than generating a schema from the code. Code-first solutions typically are limited to a single language and can create compatibility issues between the front end and other useful GraphQL tools.&lt;/p&gt; &lt;h3&gt;Separate concerns&lt;/h3&gt; &lt;p&gt;When our back- and front-end codebase is minimal we can use tools to generate code, analyze our schemas, and so on. Those tools typically do not run in production but provide a number of features missing in the reference architecture. All elements should work outside your application and can be replaced if needed.&lt;/p&gt; &lt;h3&gt;Use the GraphQL reference implementation&lt;/h3&gt; &lt;p&gt;Using the GraphQL reference implementation facilitates supportability and it is vendor agnostic. GraphQL is a Linux Foundation project with a number of reference libraries maintained under its umbrella. Choosing these libraries over single vendor and product-focused open source libraries reduces the risk of providing support and maximizes the stability of our solutions over extended periods of time.&lt;/p&gt; &lt;h3&gt;Minimalism&lt;/h3&gt; &lt;p&gt;Developers often look for libraries that offer an improved API and increase productivity. In our experience, picking a high-level tool that focuses only on the essential elements needed to build a successful GraphQL API leads to the best outcome. As a result, we've chosen to include a very short list of packages and recommendations that are useful for developers.&lt;/p&gt; &lt;h3&gt;Exclude opinionated solutions&lt;/h3&gt; &lt;p&gt;The GraphQL section of the Node.js reference architecture does not include CRUD engines or tools that affect developer flexibility and introduce proprietary APIs.&lt;/p&gt; &lt;p&gt;Based on our discussion of these principles and values, along with our prior experience, we developed the recommendations and guidance captured in the reference architecture. We hope this article has given you some insight into the background and considerations the team covered in building that section. For more information, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/graphql.md" target="_blank"&gt;GraphQL section of the Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png?itok=gJm9je_w" width="600" height="662" alt="GraphQL works!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: GraphQL works! &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/" target="_blank"&gt;Node.js reference architecture series&lt;/a&gt;. While you wait for the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you'll see the work we’ve already done and the kinds of topics you can look forward to in the future.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://www.redhat.com/en/topics/api/what-is-graphql" target="_blank"&gt;GraphQL&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/nodejs" target="_blank"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" title="Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js"&gt;Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tCiWOcx72HQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Wojciech Trocki</dc:creator><dc:date>2021-06-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs</feedburner:origLink></entry><entry><title type="html">WildFly 24 S2I images have been released on quay.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZRmyQLw3FJE/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/06/22/WildFly-s2i-24-Released/</id><updated>2021-06-22T00:00:00Z</updated><content type="html">WILDFLY 24 S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 24 have been released on . The S2I builder image has been upgraded with the following content: * . This feature-pack defines some Galleon layers to provision drivers and datasources for: MariaDB, Microsoft SQL Server, MySQL, Oracle DB and PostgreSQL. You can learn how to configure the drivers by reading the feature-pack . * adapter has been upgraded to the version 13.0.1. For a complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . HELM CHART FOR WILDFLY has been updated to use the WildFly S2I 24.0 images. The is a good place to start using Helm Chart with the WildFly S2I images. Enjoy!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZRmyQLw3FJE" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/06/22/WildFly-s2i-24-Released/</feedburner:origLink></entry><entry><title type="html">Kamelet for streaming to Kafka!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ckePpzU5Yyg/kamelet-for-streaming-to-kafka.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</id><updated>2021-06-21T14:10:00Z</updated><content type="html">You want Kafka to stream and process the data. But what comes after you set up the platform, planned the partitioning strategy, storage options, and configured the data durability? Yes! How to stream data in and out of the platform. And this is exactly what I want to discuss today.  THE BACKGROUND Before we go any further, let’s see what Kafka did to make itself blazing fast? Kafka is optimized for writing the stream data in binary format, that basically logs everything directly to the file system (Sequential I/O) and makes minimum effort to process what's in the data (Optimize for Zero Copy).  Kafka is super-charged at making sure data is stored as quickly as possible, and quickly replicating for a large number of consumers. But terrible at communication, the client that pushes content needs to SPEAK Kafka.  Here we are, having a super fast logging and distributing platform, but dumb at connecting to other data sources. So who is going to validate the data sent in/out of the kafka topic? What if I need to transform the data content? Can I filter the content partially? You guessed it. The clients.  We now need smart clients that do most of the content processing and speak Kafka at the same time.  WHAT ARE THE MOST USED CONNECT TOOLS TODAY FOR KAFKA USERS?  Kafka Connect is what the majority of the Kafka users are using today. It has been broken down into many parts such as connector, tasks, worker, converter, transformer and error handler. You can view the task and worker as how the data flow is executed. For a developer they will be mostly configuring the rest 4 pieces.  * Connector - Describes the kind of source or the sink of the data flow, translating between the client/Kafka protocol, and knowing the libraries needed.  * Converter - Converts the binary to the data format accepted by the client or vice versa   (Currently there is limited support from Confluent, they  only do data format) And does data format validation.  * Transformer - Reads into the data format, can help make simple changes to individual data chunks. Normally you would do filtering, masking or any minor changes. ( This does not support simple calculations) * Error Handler - Define a place to store problematic data (Confluent : Dead letter queues are only applicable for sink connectors.) After configuring, it then uses Task and Worker to determine how to scale and execute that pipe data in/out of Kaka. For instance, running a cluster of works to scale and allow tasks to perform parallel processing of streams.  Camel is another great option! Apache Camel is a GREAT alternative for connecting Kafka too. Here’s what Camel has to offer.  * Connector - Camel has more than 300+ connectors, you can use it to configure as source or the sink of the data flow, translating between the 100+client/Kafka protocol. * Converter -  Validate and transform data formats with simple configuration. * Transformer - Not only does simple message modification, it can apply integration patterns that are good for streaming processing, such as split, filter, even customization of processes.   * Error Handler - Dead letter queue, catching exceptions.  There are also many ways to run Camel. You can have it running as a standalone single process that directly streams data in/out of Kafka . But Kamel works EXCEPTIONALLY well on Kubernetes. It run as a cluster of instances, that execute in parallel to maximize the performance. It can be deployed as native image through Quarkus to increase density and efficiency. The platform OpenShift (Kubernetes) allows users to control the scaling of the instance. Since it’s on K8s, another advantage is that operation can operate these as a unify platform, along with all other microservices.  WHY KAMELET?  (THIS IS THE WAY!) One of the biggest hurdles for non Camel developers is, they need to learn another framework, maybe another language (Non-Java) to be able to get Camel running. What if we can smooth the learning curve and make it simple for newcomers? We see a great number of use cases where the masking and filtering are implemented company wide. Being able to build a repository and reuse these logics will make developers work more efficiently.  PLUG &amp;amp; PLAY  You can look at Kamelets as templates, where you can define where to consume data from and send data to, does filtering, masking, simple calculation logic. Once the template is defined, it can be made available to the teams, that simply plugs it into the platform, configure for their needs (with either Kamelet Binding or another Camel route), and boom. The underlying Camel K will do the hard work for you, compile, build, package and deploy. You have a smart running data pipeline streams into Kafka.  ASSEMBLE &amp;amp; REUSE In a data pipeline, sometimes, you just need that bit of extra work on the data. Instead of defining a single template for each case, you can also break it down into smaller tasks. And assemble these small tasks to perform in the pipeline for each use case. .    STREAMS &amp;amp; SERVERLESS Kamelets allows you to stream data to/from either Kafka store or Knative event channel/broker. To be able to support Knative, Kamelet can help translate messages to CloudEvents, which is the CNCF standard event format for serverless. And also apply any pre/post-processing of the content in the pipeline.  SCALABLE &amp;amp; FLEXIBLE Kamelet lives on Kubernetes(can also run standalone), which gives you a comprehensive set of scaling tools, readiness, liveness check and scaling configuration. They are all part of the package. It scales by adding more instances. The UI on the OpenShift Developer Console can assist you to fill in what’s needed. And also auto discover the available source/sink for you to choose where the data pipelines start or end.  UNIFY FOR DEV &amp;amp; OPS  In many cases, DevOps engineers are often required to develop another set of automation tools for the deployment of connectors. Kamelet can run like other applications on kubernetes, the same tools can be used to build, deploy and monitor these pipelines. The streamline DEVOPS experience can help speed up the automation setup time.   MARKETPLACE List of catalogues that are already available  (Not enough?). If you just want to stream data directly, simple pick the ones you need and start streaming. And we welcome your contributions too. What to know more about Kamelet? Take a look at this video, where it talks about why using Kamelet for streaming data to Kafka with a demo.   Introduction  What is Kamelet?   Why do you need connectors to Kafka, and what is required in each connector?   Why Kamelet?   Marketplace of Kamelet!   Using Kamelet as a Kafka user   Building a Kamelet   Running Kamelet on Kubernetes   Demo   Red Hat OpenShift Streams in action   Kamelets in action&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ckePpzU5Yyg" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</feedburner:origLink></entry><entry><title type="html">An Infinispan .Net Core client over the Hot Rod protocol</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/W4mNLc6TGBA/infinispan-dotnet-core-client" /><author><name>Vittorio Rigamonti</name></author><id>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</id><updated>2021-06-21T12:00:00Z</updated><content type="html">Dear Infinispanners, The Infinispan team would like to share a new project we’re working on: . Our current .NET client is based on the C++ core implementation, which is a solution that has its pros and cons. It makes it easier to guarantee parity between the C++ and C# clients over the time, but has the drawback of tying clients to specific a architecture. In turn that complicates portability and distribution, making the release lifecycle for those clients more onerous and sluggish. The is a 100% C# Hot Rod client designed with the aim of being portable across systems via the .Net Core platform and easier to deploy and consume via the Nuget platform. If you are involved with the .NET Core ecosystem we hope you will find this project of interest. Entry points for the project are: * , current status and news; * , this will contain the same testsuite of the current project with the aim of making it easier to compare with the .NET core client as it matures. * , an example of project which uses the client package. * is where the package is published. Hope this will makes C# developers happy! Please let us know your thoughts, a good place for them is the page. The Infinispan Team&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/W4mNLc6TGBA" height="1" width="1" alt=""/&gt;</content><dc:creator>Vittorio Rigamonti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</feedburner:origLink></entry><entry><title>Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CYg36HRkzXs/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" /><author><name>Ken Lee, Preska Sharma, Keyvan Pishevar</name></author><id>e9039812-d068-4304-8860-390fbe4ac2a2</id><updated>2021-06-21T07:00:00Z</updated><published>2021-06-21T07:00:00Z</published><summary type="html">&lt;p&gt;Our team recently created an application called Beer Horoscope, which we used to illustrate the extensive possibilities for modern software development and deployment with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and community-built free software tools. The application's front end collects user preferences and makes beer recommendations. The back end performs &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; on users and products (beers) to make appropriate recommendations. Figure 1 shows how we combined an &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt; with machine learning models that are applicable to numerous real-world scenarios.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_cycle.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_cycle.png?itok=5PegWzRq" width="600" height="209" alt="In the Beer Horoscope application, we run a cycle of analyzing new ratings, retraining models, and redeploying the service." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The application flow of data collection, analysis, and service redeployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This article summarizes our talk at the 2021 Red Hat Summit break-out session titled &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog/session/1607116397279001IHS4"&gt;Modern Fortune Teller: Your Beer Horoscope with AI/ML&lt;/a&gt;. We'll discuss how we used &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it. We'll also explain how we used &lt;a href="https://opendatahub.io"&gt;Open Data Hub&lt;/a&gt; as a one-stop machine learning environment to create and test our algorithms on OpenShift. See our &lt;a href="https://github.com/beer-horoscope/beer-horoscope"&gt;GitHub repository&lt;/a&gt; for application source code and additional documentation.&lt;/p&gt; &lt;h2&gt;What is GitOps?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;GitOps&lt;/a&gt; is a way of implementing &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous deployment&lt;/a&gt; (CD) for cloud-native applications. GitOps extends CD from development to cloud deployment using tools developers are already familiar with, including Git.&lt;/p&gt; &lt;p&gt;The core idea of GitOps is to create a Git repository that contains declarative descriptions of the infrastructure. These are updated so they always indicate the images currently desired in the production environment, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/gitops-flow.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/gitops-flow.png?itok=seT3Gs50" width="392" height="414" alt="GitOps continuously evaluates the desired state of production systems and automatically updates those systems in the cloud." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Flow of events in GitOps that keeps production systems up to date in the cloud. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;GitOps's advantages in practice include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Faster and more frequent application deployments&lt;/li&gt; &lt;li&gt;Easier and faster error recovery&lt;/li&gt; &lt;li&gt;Simplified credential management&lt;/li&gt; &lt;li&gt;Self-documenting deployments&lt;/li&gt; &lt;li&gt;Shared knowledge throughout the team&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Red Hat's GitOps Operator&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift is a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; platform meeting the declarative principles that &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/gitops/understanding-openshift-gitops.html"&gt;allow administrators to configure and manage deployments using GitOps&lt;/a&gt;. Working within a Kubernetes-based infrastructure and applications, you can apply consistency across clusters and development life cycles.&lt;/p&gt; &lt;p&gt;Red Hat collaborates with open source projects such as &lt;a href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt; and &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton Pipeline&lt;/a&gt; to implement a framework for GitOps.&lt;/p&gt; &lt;p&gt;For this application, we leveraged Red Hat's &lt;a href="https://github.com/redhat-developer/gitops-operator"&gt;GitOps Operator&lt;/a&gt; for application deployment. This operator allows for continuous updates and delivery via ArgoCD and Git, thus implementing GitOps.&lt;/p&gt; &lt;p&gt;ArgoCD pulls the deployment instructions from our Git repository and installs &lt;a href="https://developers.redhat.com/products/amq/getting-started"&gt;Red Hat AMQ Streams&lt;/a&gt;. AMQ Streams is based on the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; streaming tool. The AMQ Streams Operator allows developers to use Kafka and various components, such as &lt;a href="https://docs.confluent.io/home/connect/overview.html"&gt;Kafka Connectors&lt;/a&gt;, to support complex event processing. For this project, we use AMQ Streams in high availability mode.&lt;/p&gt; &lt;h2&gt;The Open Data Hub Operator&lt;/h2&gt; &lt;p&gt;We used Open Data Hub as a one-stop environment for machine learning and artificial intelligence (&lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt;) services and tools on OpenShift. Open Data Hub provides tools at every stage of the AI/ML workflow, and for multiple user personas including data scientists, DevOps engineers, and software engineers. The &lt;a href="https://gitlab.com/opendatahub/opendatahub-operator"&gt;Open Data Hub Operator&lt;/a&gt; (Figure 3) lets developers use a best-of-breed machine learning toolset and focus on building the application.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/open_data_hub.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/open_data_hub.png?itok=OkszwMB-" width="600" height="339" alt="The Open Data Hub Operator gave the Beer Horoscope project access to a range of tools, including operators for AMQ Streams, Prometheus, and others." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Tools provided by the Open Data Hub Operator to the Beer Horoscope project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 4 shows the platform components, such as Jupyter Notebook, Apache Spark, and others, that Open Data Hub makes available for data scientists via Kubeflow.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kfdef.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kfdef.png?itok=nqIBL8jy" width="600" height="395" alt="Kubeflow provides a wide range of open source tools for data processing and analysis." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Data processing and analysis tools provided by Kubeflow. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Developing the recommendation system&lt;/h2&gt; &lt;p&gt;To develop the algorithms for the Beer Horoscope project, we needed a recommendation system. To choose the correct algorithms for the system, we first had to determine the relationships involved when users get a beer recommendation. Three main types of relationships occur in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User-product: What kind of beer does this user like to drink?&lt;/li&gt; &lt;li&gt;Product-product: What beers are similar to each other?&lt;/li&gt; &lt;li&gt;User-user: Which users have similar taste in beer?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Two of these relationships can be established by two very popular algorithms used in recommendation systems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Collaborative filtering&lt;/strong&gt;: Used to establish user-user relationships. If one user rates Beer A very highly, and another user also rates Beer A very highly, we can assume these users have similar taste in beer. We can then start recommending to each user the beers that are highly rated by the other user.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content-based filtering&lt;/strong&gt;: Used to establish product-product relationships. If a user likes Beer A, it's safe to recommend beers similar to Beer A to the user.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We developed models using these two algorithms on &lt;a href="https://jupyter.org/hub"&gt;JupyterHub&lt;/a&gt; through Open Data Hub. We had access there to all of the components that made up the development environment, including environment variables, databases, configurations settings, and so on.&lt;/p&gt; &lt;p&gt;Once we created our models, our application deployment life cycle became very similar to the software development life cycle.&lt;/p&gt; &lt;p&gt;Next, we'll take a look at how cloud-native development utilizes containers, &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt;, and &lt;a href="topics/microservices"&gt;microservices&lt;/a&gt; to automate these formerly time-consuming steps.&lt;/p&gt; &lt;h2&gt;How we built the Beer Horoscope&lt;/h2&gt; &lt;p&gt;Up to this point, we've covered how we automated the infrastructure that our application runs on, from the perspective of a &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; engineer, by leveraging GitOps. We then discussed how we trained and created the data models from the perspective of a data engineer and data scientist.&lt;/p&gt; &lt;p&gt;Here, we turn to the viewpoint of an application developer. We'll go over how an application interacts with trained data models and how to leverage the OpenShift infrastructure and platform to make these interfaces possible.&lt;/p&gt; &lt;p&gt;The AMQ Streams Operator, GitOps Operator, and Open Data Hub Operator, discussed earlier, were all available from the OperatorHub, shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/operatorhub_tools.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/operatorhub_tools.png?itok=6aUGErRS" width="600" height="613" alt="A number of open source, community-based tools came together in Open Data Hub and are exposed by OpenShift as Operators in OperatorHub." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Community-based tools that contribute to OperatorHub. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Application architecture&lt;/h2&gt; &lt;p&gt;Figure 6 lists the systems involved in this architecture and roughly indicates how they relate to each other. The application components are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Users&lt;/strong&gt;: These are evaluated for their preferences and are served by the project.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Applications and services tier&lt;/strong&gt;: This collection typically houses artifacts such as web applications, REST services, and internal services. Much of the APIs and underlying business logic were extrapolated from Jupyter notebooks developed by data engineers. The front-end component was written using &lt;a href="https://vuejs.org/"&gt;Vue.js&lt;/a&gt;. The API services are built on &lt;a href="search?t=python"&gt;Python&lt;/a&gt; and &lt;a href="https://flask.palletsprojects.com/"&gt;Flask&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data tier&lt;/strong&gt;: This tier stores both raw and structured data, as well as trained data models. This data is used by the consuming tiers. The Beer Horoscope application uses both the &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; relational database and file storage to store data.&lt;/li&gt; &lt;li&gt;Event-processing tier: In this tier, we orchestrate how we process any new data introduced into our ecosystem. We create data streams to handle complex event processing scenarios and business rules. For this tier, we used Kafka Connectors and &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Streams&lt;/a&gt; for complex event processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logging, monitoring, and analytics&lt;/strong&gt;: This tier provides functions for logging and monitoring, so that we can analyze what's going on in real time and keep historical records. This tier used the &lt;a href="https://operatorhub.io/operator/grafana-operator"&gt;Grafana&lt;/a&gt; and &lt;a href="https://operatorhub.io/operator/prometheus"&gt;Prometheus&lt;/a&gt; Operators.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container registry&lt;/strong&gt;: To facilitate the versioning, storage, and retrieval of container image artifacts in our OpenShift cluster, we stored all application image artifacts within a container registry. The Beer Horoscope uses &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt; to host these artifacts.&lt;/li&gt; &lt;/ul&gt;&lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_architecture.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_architecture.png?itok=6mm1TBRz" width="600" height="384" alt="The Beer Horoscope application includes many components and tiers, described in the text." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Components of the Beer Horoscope application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored the tools and methods we used to operationalize our machine learning models, and how we then brought algorithms written on a Jupyter notebook into production through a user-friendly web application.&lt;/p&gt; &lt;p&gt;Before starting a full-stack project, you need an environment that supports continuous delivery. We used Red Hat's GitOps Operator on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it.&lt;/p&gt; &lt;p&gt;Then, we used OpenShift's Open Data Hub Operator as a one-stop machine learning environment to create and test our algorithms. The MySQL databases and file system store our large datasets and trained data models, respectively.&lt;/p&gt; &lt;p&gt;Finally, we created an application that interacts with our models. Our full-stack application includes the front-end user interface, API services written in Flask that talk to our machine-learning model training services written in Python, the data tier, and the event-processing tier that uses Kakfa Streams through AMQ Streams.&lt;/p&gt; &lt;p&gt;By closely examining and optimizing the software development cycle, we were able to collaborate and deploy into production an intelligent application—one that's telling you to go grab a cold beer right now!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" title="Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CYg36HRkzXs" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ken Lee, Preska Sharma, Keyvan Pishevar</dc:creator><dc:date>2021-06-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat</feedburner:origLink></entry><entry><title type="html">Cloud adoption - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2JAH4BfNY0A/cloud-adoption-common-architectural-elemetns.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</id><updated>2021-06-21T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements In our  from this series we introduced a use case around cloud adoption for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the architectural details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the cloud adoption architecture. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architecture.  It's our intent to provide an example for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architecture generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. CORE DATA CENTER The logical view splits this solution space into several identifiable collections where the cloud adoption solution is laid out. These logical collections ensure that your organisation can provide effective automation for deploying and managing workloads across multiple cloud infrastructures according to performance, security, compliance, and cost requirements. The first collection on the left is tagged as the core data center and holds all the logical elements needed to put together the images for your infrastructure and workloads to run on. You find a source code management (SCM) system, an image store, and the server image build pipeline. All elements used by an organisation to create, manage, store, and testing images for distribution. INFRASTRUCTURE MANAGEMENT The infrastructure management collection is where intelligence is gathered, monitoring is performed, and based on the findings, triggers automated reactions and orchestrates updates to your infrastructure anywhere in your organisation. A smart management element is used for tracking, managing, auditing, and collecting data on your entire infrastructure to ensure that baselines are met. Based on your choices and the results of data collected, your insights trigger corrections, updates, or even rolling out of new infrastructure across any of the cloud infrastructure your organisation might be using. The automation orchestration element is tasked with orchestration of infrastructure tasks in a fully automated and pre-tested fashion. This element is directed to execute certain tasks in a certain order based on the findings of the smart management element. CLOUD INFRASTRUCTURE This collection of elements are all aspects of an organisations cloud infrastructure. The idea is that organisations are at the very least moving to put the cloud-native experience together for their development teams to execute on their business goals while supporting an agile customer experience.  To provide a cloud experience, existing physical data center resources might be the starting point to be offered to the organisation with a cloud-like experience. Once this is completed, the organisation then has a private cloud to build, test, and run its workloads on.  Finally, as needed, organisations can expand services and applications out into one or more of the public cloud providers. All of these are shown here as a Red Hat Enterprise Linux (RHEL) host element with an image registry to facilitate the deployment of infrastructure, services, and applications across the entire hybrid cloud infrastructure. CLOUD SERVICES Last but not least, there is a need for cloud services that can facilitate all it takes to span the monitoring, analysing, and deployment of an organisations workloads across their hybrid cloud infrastructure. The first element is that of enterprise operating automation which facilitates consistent, repeatable, and tested infrastructure automation tasks as needed by the other elements managing the hybrid cloud infrastructure.  Next, there is the insights platform. This is key to monitoring and data collection around the entire hybrid cloud infrastructure. Based on this data and working together with insights services, automated actions can take place around updates, security patches, infrastructure rollouts, workload management, and workload migrations. This is the key to an organisations ability to successfully adopt a truly hybrid cloud infrastructure. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the cloud adoption use case.  An overview of this series on the cloud adoption portfolio architecture: 1. 2. 3. Example adoption architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example adoption architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2JAH4BfNY0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</feedburner:origLink></entry></feed>

<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Calling All Roadies for the Quarkus World Tour</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/llNkBa0D-m8/" /><author><name /></author><id>https://quarkus.io/blog/calling-all-roadies/</id><updated>2021-06-22T00:00:00Z</updated><content type="html">The Quarkus World Tour kicked off in March to provide a unique, hands-on Quarkus experience for Java developers across the globe. The goal of the tour is to introduce Quarkus to Java developers and set them down the path of creating applications and participating in the community. Since the tour...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/llNkBa0D-m8" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/calling-all-roadies/</feedburner:origLink></entry><entry><title type="html">Kamelet for streaming to Kafka!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ckePpzU5Yyg/kamelet-for-streaming-to-kafka.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</id><updated>2021-06-21T14:10:00Z</updated><content type="html">You want Kafka to stream and process the data. But what comes after you set up the platform, planned the partitioning strategy, storage options, and configured the data durability? Yes! How to stream data in and out of the platform. And this is exactly what I want to discuss today.  THE BACKGROUND Before we go any further, let’s see what Kafka did to make itself blazing fast? Kafka is optimized for writing the stream data in binary format, that basically logs everything directly to the file system (Sequential I/O) and makes minimum effort to process what's in the data (Optimize for Zero Copy).  Kafka is super-charged at making sure data is stored as quickly as possible, and quickly replicating for a large number of consumers. But terrible at communication, the client that pushes content needs to SPEAK Kafka.  Here we are, having a super fast logging and distributing platform, but dumb at connecting to other data sources. So who is going to validate the data sent in/out of the kafka topic? What if I need to transform the data content? Can I filter the content partially? You guessed it. The clients.  We now need smart clients that do most of the content processing and speak Kafka at the same time.  WHAT ARE THE MOST USED CONNECT TOOLS TODAY FOR KAFKA USERS?  Kafka Connect is what the majority of the Kafka users are using today. It has been broken down into many parts such as connector, tasks, worker, converter, transformer and error handler. You can view the task and worker as how the data flow is executed. For a developer they will be mostly configuring the rest 4 pieces.  * Connector - Describes the kind of source or the sink of the data flow, translating between the client/Kafka protocol, and knowing the libraries needed.  * Converter - Converts the binary to the data format accepted by the client or vice versa   (Currently there is limited support from Confluent, they  only do data format) And does data format validation.  * Transformer - Reads into the data format, can help make simple changes to individual data chunks. Normally you would do filtering, masking or any minor changes. ( This does not support simple calculations) * Error Handler - Define a place to store problematic data (Confluent : Dead letter queues are only applicable for sink connectors.) After configuring, it then uses Task and Worker to determine how to scale and execute that pipe data in/out of Kaka. For instance, running a cluster of works to scale and allow tasks to perform parallel processing of streams.  Camel is another great option! Apache Camel is a GREAT alternative for connecting Kafka too. Here’s what Camel has to offer.  * Connector - Camel has more than 300+ connectors, you can use it to configure as source or the sink of the data flow, translating between the 100+client/Kafka protocol. * Converter -  Validate and transform data formats with simple configuration. * Transformer - Not only does simple message modification, it can apply integration patterns that are good for streaming processing, such as split, filter, even customization of processes.   * Error Handler - Dead letter queue, catching exceptions.  There are also many ways to run Camel. You can have it running as a standalone single process that directly streams data in/out of Kafka . But Kamel works EXCEPTIONALLY well on Kubernetes. It run as a cluster of instances, that execute in parallel to maximize the performance. It can be deployed as native image through Quarkus to increase density and efficiency. The platform OpenShift (Kubernetes) allows users to control the scaling of the instance. Since it’s on K8s, another advantage is that operation can operate these as a unify platform, along with all other microservices.  WHY KAMELET?  (THIS IS THE WAY!) One of the biggest hurdles for non Camel developers is, they need to learn another framework, maybe another language (Non-Java) to be able to get Camel running. What if we can smooth the learning curve and make it simple for newcomers? We see a great number of use cases where the masking and filtering are implemented company wide. Being able to build a repository and reuse these logics will make developers work more efficiently.  PLUG &amp;amp; PLAY  You can look at Kamelets as templates, where you can define where to consume data from and send data to, does filtering, masking, simple calculation logic. Once the template is defined, it can be made available to the teams, that simply plugs it into the platform, configure for their needs (with either Kamelet Binding or another Camel route), and boom. The underlying Camel K will do the hard work for you, compile, build, package and deploy. You have a smart running data pipeline streams into Kafka.  ASSEMBLE &amp;amp; REUSE In a data pipeline, sometimes, you just need that bit of extra work on the data. Instead of defining a single template for each case, you can also break it down into smaller tasks. And assemble these small tasks to perform in the pipeline for each use case. .    STREAMS &amp;amp; SERVERLESS Kamelets allows you to stream data to/from either Kafka store or Knative event channel/broker. To be able to support Knative, Kamelet can help translate messages to CloudEvents, which is the CNCF standard event format for serverless. And also apply any pre/post-processing of the content in the pipeline.  SCALABLE &amp;amp; FLEXIBLE Kamelet lives on Kubernetes(can also run standalone), which gives you a comprehensive set of scaling tools, readiness, liveness check and scaling configuration. They are all part of the package. It scales by adding more instances. The UI on the OpenShift Developer Console can assist you to fill in what’s needed. And also auto discover the available source/sink for you to choose where the data pipelines start or end.  UNIFY FOR DEV &amp;amp; OPS  In many cases, DevOps engineers are often required to develop another set of automation tools for the deployment of connectors. Kamelet can run like other applications on kubernetes, the same tools can be used to build, deploy and monitor these pipelines. The streamline DEVOPS experience can help speed up the automation setup time.   MARKETPLACE List of catalogues that are already available  (Not enough?). If you just want to stream data directly, simple pick the ones you need and start streaming. And we welcome your contributions too. What to know more about Kamelet? Take a look at this video, where it talks about why using Kamelet for streaming data to Kafka with a demo.   Introduction  What is Kamelet?   Why do you need connectors to Kafka, and what is required in each connector?   Why Kamelet?   Marketplace of Kamelet!   Using Kamelet as a Kafka user   Building a Kamelet   Running Kamelet on Kubernetes   Demo   Red Hat OpenShift Streams in action   Kamelets in action&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ckePpzU5Yyg" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</feedburner:origLink></entry><entry><title>Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CYg36HRkzXs/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" /><author><name>Ken Lee, Preska Sharma, Keyvan Pishevar</name></author><id>e9039812-d068-4304-8860-390fbe4ac2a2</id><updated>2021-06-21T07:00:00Z</updated><published>2021-06-21T07:00:00Z</published><summary type="html">&lt;p&gt;Our team recently created an application called Beer Horoscope, which we used to illustrate the extensive possibilities for modern software development and deployment with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and community-built free software tools. The application's front end collects user preferences and makes beer recommendations. The back end performs &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; on users and products (beers) to make appropriate recommendations. Figure 1 shows how we combined an &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt; with machine learning models that are applicable to numerous real-world scenarios.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_cycle.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_cycle.png?itok=5PegWzRq" width="600" height="209" alt="In the Beer Horoscope application, we run a cycle of analyzing new ratings, retraining models, and redeploying the service." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The application flow of data collection, analysis, and service redeployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This article summarizes our talk at the 2021 Red Hat Summit break-out session titled &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog/session/1607116397279001IHS4"&gt;Modern Fortune Teller: Your Beer Horoscope with AI/ML&lt;/a&gt;. We'll discuss how we used &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it. We'll also explain how we used &lt;a href="https://opendatahub.io"&gt;Open Data Hub&lt;/a&gt; as a one-stop machine learning environment to create and test our algorithms on OpenShift. See our &lt;a href="https://github.com/beer-horoscope/beer-horoscope"&gt;GitHub repository&lt;/a&gt; for application source code and additional documentation.&lt;/p&gt; &lt;h2&gt;What is GitOps?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;GitOps&lt;/a&gt; is a way of implementing &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous deployment&lt;/a&gt; (CD) for cloud-native applications. GitOps extends CD from development to cloud deployment using tools developers are already familiar with, including Git.&lt;/p&gt; &lt;p&gt;The core idea of GitOps is to create a Git repository that contains declarative descriptions of the infrastructure. These are updated so they always indicate the images currently desired in the production environment, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/gitops-flow.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/gitops-flow.png?itok=seT3Gs50" width="392" height="414" alt="GitOps continuously evaluates the desired state of production systems and automatically updates those systems in the cloud." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Flow of events in GitOps that keeps production systems up to date in the cloud. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;GitOps's advantages in practice include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Faster and more frequent application deployments&lt;/li&gt; &lt;li&gt;Easier and faster error recovery&lt;/li&gt; &lt;li&gt;Simplified credential management&lt;/li&gt; &lt;li&gt;Self-documenting deployments&lt;/li&gt; &lt;li&gt;Shared knowledge throughout the team&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Red Hat's GitOps Operator&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift is a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; platform meeting the declarative principles that &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/gitops/understanding-openshift-gitops.html"&gt;allow administrators to configure and manage deployments using GitOps&lt;/a&gt;. Working within a Kubernetes-based infrastructure and applications, you can apply consistency across clusters and development life cycles.&lt;/p&gt; &lt;p&gt;Red Hat collaborates with open source projects such as &lt;a href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt; and &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton Pipeline&lt;/a&gt; to implement a framework for GitOps.&lt;/p&gt; &lt;p&gt;For this application, we leveraged Red Hat's &lt;a href="https://github.com/redhat-developer/gitops-operator"&gt;GitOps Operator&lt;/a&gt; for application deployment. This operator allows for continuous updates and delivery via ArgoCD and Git, thus implementing GitOps.&lt;/p&gt; &lt;p&gt;ArgoCD pulls the deployment instructions from our Git repository and installs &lt;a href="https://developers.redhat.com/products/amq/getting-started"&gt;Red Hat AMQ Streams&lt;/a&gt;. AMQ Streams is based on the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; streaming tool. The AMQ Streams Operator allows developers to use Kafka and various components, such as &lt;a href="https://docs.confluent.io/home/connect/overview.html"&gt;Kafka Connectors&lt;/a&gt;, to support complex event processing. For this project, we use AMQ Streams in high availability mode.&lt;/p&gt; &lt;h2&gt;The Open Data Hub Operator&lt;/h2&gt; &lt;p&gt;We used Open Data Hub as a one-stop environment for machine learning and artificial intelligence (&lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt;) services and tools on OpenShift. Open Data Hub provides tools at every stage of the AI/ML workflow, and for multiple user personas including data scientists, DevOps engineers, and software engineers. The &lt;a href="https://gitlab.com/opendatahub/opendatahub-operator"&gt;Open Data Hub Operator&lt;/a&gt; (Figure 3) lets developers use a best-of-breed machine learning toolset and focus on building the application.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/open_data_hub.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/open_data_hub.png?itok=OkszwMB-" width="600" height="339" alt="The Open Data Hub Operator gave the Beer Horoscope project access to a range of tools, including operators for AMQ Streams, Prometheus, and others." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Tools provided by the Open Data Hub Operator to the Beer Horoscope project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 4 shows the platform components, such as Jupyter Notebook, Apache Spark, and others, that Open Data Hub makes available for data scientists via Kubeflow.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kfdef.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kfdef.png?itok=nqIBL8jy" width="600" height="395" alt="Kubeflow provides a wide range of open source tools for data processing and analysis." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Data processing and analysis tools provided by Kubeflow. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Developing the recommendation system&lt;/h2&gt; &lt;p&gt;To develop the algorithms for the Beer Horoscope project, we needed a recommendation system. To choose the correct algorithms for the system, we first had to determine the relationships involved when users get a beer recommendation. Three main types of relationships occur in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User-product: What kind of beer does this user like to drink?&lt;/li&gt; &lt;li&gt;Product-product: What beers are similar to each other?&lt;/li&gt; &lt;li&gt;User-user: Which users have similar taste in beer?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Two of these relationships can be established by two very popular algorithms used in recommendation systems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Collaborative filtering&lt;/strong&gt;: Used to establish user-user relationships. If one user rates Beer A very highly, and another user also rates Beer A very highly, we can assume these users have similar taste in beer. We can then start recommending to each user the beers that are highly rated by the other user.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content-based filtering&lt;/strong&gt;: Used to establish product-product relationships. If a user likes Beer A, it's safe to recommend beers similar to Beer A to the user.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We developed models using these two algorithms on &lt;a href="https://jupyter.org/hub"&gt;JupyterHub&lt;/a&gt; through Open Data Hub. We had access there to all of the components that made up the development environment, including environment variables, databases, configurations settings, and so on.&lt;/p&gt; &lt;p&gt;Once we created our models, our application deployment life cycle became very similar to the software development life cycle.&lt;/p&gt; &lt;p&gt;Next, we'll take a look at how cloud-native development utilizes containers, &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt;, and &lt;a href="topics/microservices"&gt;microservices&lt;/a&gt; to automate these formerly time-consuming steps.&lt;/p&gt; &lt;h2&gt;How we built the Beer Horoscope&lt;/h2&gt; &lt;p&gt;Up to this point, we've covered how we automated the infrastructure that our application runs on, from the perspective of a &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; engineer, by leveraging GitOps. We then discussed how we trained and created the data models from the perspective of a data engineer and data scientist.&lt;/p&gt; &lt;p&gt;Here, we turn to the viewpoint of an application developer. We'll go over how an application interacts with trained data models and how to leverage the OpenShift infrastructure and platform to make these interfaces possible.&lt;/p&gt; &lt;p&gt;The AMQ Streams Operator, GitOps Operator, and Open Data Hub Operator, discussed earlier, were all available from the OperatorHub, shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/operatorhub_tools.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/operatorhub_tools.png?itok=6aUGErRS" width="600" height="613" alt="A number of open source, community-based tools came together in Open Data Hub and are exposed by OpenShift as Operators in OperatorHub." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Community-based tools that contribute to OperatorHub. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Application architecture&lt;/h2&gt; &lt;p&gt;Figure 6 lists the systems involved in this architecture and roughly indicates how they relate to each other. The application components are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Users&lt;/strong&gt;: These are evaluated for their preferences and are served by the project.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Applications and services tier&lt;/strong&gt;: This collection typically houses artifacts such as web applications, REST services, and internal services. Much of the APIs and underlying business logic were extrapolated from Jupyter notebooks developed by data engineers. The front-end component was written using &lt;a href="https://vuejs.org/"&gt;Vue.js&lt;/a&gt;. The API services are built on &lt;a href="search?t=python"&gt;Python&lt;/a&gt; and &lt;a href="https://flask.palletsprojects.com/"&gt;Flask&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data tier&lt;/strong&gt;: This tier stores both raw and structured data, as well as trained data models. This data is used by the consuming tiers. The Beer Horoscope application uses both the &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; relational database and file storage to store data.&lt;/li&gt; &lt;li&gt;Event-processing tier: In this tier, we orchestrate how we process any new data introduced into our ecosystem. We create data streams to handle complex event processing scenarios and business rules. For this tier, we used Kafka Connectors and &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Streams&lt;/a&gt; for complex event processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logging, monitoring, and analytics&lt;/strong&gt;: This tier provides functions for logging and monitoring, so that we can analyze what's going on in real time and keep historical records. This tier used the &lt;a href="https://operatorhub.io/operator/grafana-operator"&gt;Grafana&lt;/a&gt; and &lt;a href="https://operatorhub.io/operator/prometheus"&gt;Prometheus&lt;/a&gt; Operators.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container registry&lt;/strong&gt;: To facilitate the versioning, storage, and retrieval of container image artifacts in our OpenShift cluster, we stored all application image artifacts within a container registry. The Beer Horoscope uses &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt; to host these artifacts.&lt;/li&gt; &lt;/ul&gt;&lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_architecture.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_architecture.png?itok=6mm1TBRz" width="600" height="384" alt="The Beer Horoscope application includes many components and tiers, described in the text." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Components of the Beer Horoscope application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored the tools and methods we used to operationalize our machine learning models, and how we then brought algorithms written on a Jupyter notebook into production through a user-friendly web application.&lt;/p&gt; &lt;p&gt;Before starting a full-stack project, you need an environment that supports continuous delivery. We used Red Hat's GitOps Operator on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it.&lt;/p&gt; &lt;p&gt;Then, we used OpenShift's Open Data Hub Operator as a one-stop machine learning environment to create and test our algorithms. The MySQL databases and file system store our large datasets and trained data models, respectively.&lt;/p&gt; &lt;p&gt;Finally, we created an application that interacts with our models. Our full-stack application includes the front-end user interface, API services written in Flask that talk to our machine-learning model training services written in Python, the data tier, and the event-processing tier that uses Kakfa Streams through AMQ Streams.&lt;/p&gt; &lt;p&gt;By closely examining and optimizing the software development cycle, we were able to collaborate and deploy into production an intelligent application—one that's telling you to go grab a cold beer right now!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" title="Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CYg36HRkzXs" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ken Lee, Preska Sharma, Keyvan Pishevar</dc:creator><dc:date>2021-06-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat</feedburner:origLink></entry><entry><title type="html">Cloud adoption - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2JAH4BfNY0A/cloud-adoption-common-architectural-elemetns.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</id><updated>2021-06-21T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements In our  from this series we introduced a use case around cloud adoption for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the architectural details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the cloud adoption architecture. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architecture.  It's our intent to provide an example for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architecture generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. CORE DATA CENTER The logical view splits this solution space into several identifiable collections where the cloud adoption solution is laid out. These logical collections ensure that your organisation can provide effective automation for deploying and managing workloads across multiple cloud infrastructures according to performance, security, compliance, and cost requirements. The first collection on the left is tagged as the core data center and holds all the logical elements needed to put together the images for your infrastructure and workloads to run on. You find a source code management (SCM) system, an image store, and the server image build pipeline. All elements used by an organisation to create, manage, store, and testing images for distribution. INFRASTRUCTURE MANAGEMENT The infrastructure management collection is where intelligence is gathered, monitoring is performed, and based on the findings, triggers automated reactions and orchestrates updates to your infrastructure anywhere in your organisation. A smart management element is used for tracking, managing, auditing, and collecting data on your entire infrastructure to ensure that baselines are met. Based on your choices and the results of data collected, your insights trigger corrections, updates, or even rolling out of new infrastructure across any of the cloud infrastructure your organisation might be using. The automation orchestration element is tasked with orchestration of infrastructure tasks in a fully automated and pre-tested fashion. This element is directed to execute certain tasks in a certain order based on the findings of the smart management element. CLOUD INFRASTRUCTURE This collection of elements are all aspects of an organisations cloud infrastructure. The idea is that organisations are at the very least moving to put the cloud-native experience together for their development teams to execute on their business goals while supporting an agile customer experience.  To provide a cloud experience, existing physical data center resources might be the starting point to be offered to the organisation with a cloud-like experience. Once this is completed, the organisation then has a private cloud to build, test, and run its workloads on.  Finally, as needed, organisations can expand services and applications out into one or more of the public cloud providers. All of these are shown here as a Red Hat Enterprise Linux (RHEL) host element with an image registry to facilitate the deployment of infrastructure, services, and applications across the entire hybrid cloud infrastructure. CLOUD SERVICES Last but not least, there is a need for cloud services that can facilitate all it takes to span the monitoring, analysing, and deployment of an organisations workloads across their hybrid cloud infrastructure. The first element is that of enterprise operating automation which facilitates consistent, repeatable, and tested infrastructure automation tasks as needed by the other elements managing the hybrid cloud infrastructure.  Next, there is the insights platform. This is key to monitoring and data collection around the entire hybrid cloud infrastructure. Based on this data and working together with insights services, automated actions can take place around updates, security patches, infrastructure rollouts, workload management, and workload migrations. This is the key to an organisations ability to successfully adopt a truly hybrid cloud infrastructure. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the cloud adoption use case.  An overview of this series on the cloud adoption portfolio architecture: 1. 2. 3. Example adoption architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example adoption architecture. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2JAH4BfNY0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</feedburner:origLink></entry><entry><title>Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w5sV_LpWTN0/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry" /><author><name>Jaideep Rao</name></author><id>764338ac-d9a4-4733-a077-4936b2d6274d</id><updated>2021-06-18T07:00:00Z</updated><published>2021-06-18T07:00:00Z</published><summary type="html">&lt;p&gt;Typically, building &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and images from a standard &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;Dockerfile&lt;/a&gt; requires root access and permissions. This can create a challenge when working with public or shared clusters. For example, cluster admins don't often allow permissions to run this type of workload, as it might compromise the security of the entire cluster.&lt;/p&gt; &lt;p&gt;In these situations, many developers use a build tool such as &lt;a href="https://github.com/GoogleContainerTools/kaniko"&gt;kaniko&lt;/a&gt; to simplify the effort. Kaniko can build your images without requiring root access. This capability makes kaniko a feasible alternative for building containers and images in any kind of environment; for example, standard &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, Google Kubernetes Engine, and public or shared clusters. Kaniko can also automatically push your images to a specified image registry.&lt;/p&gt; &lt;p&gt;This article shows you how to use kaniko to build a container image in a &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster and push the image to a registry.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To perform a kaniko build on a Red Hat OpenShift cluster, ensure that the following prerequisites are in place:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Access to an active OpenShift cluster (with admin access).&lt;/li&gt; &lt;li&gt;Access to a source code repository that is either local or hosted somewhere, such as GitHub.&lt;/li&gt; &lt;li&gt;A valid Dockerfile for your target source directory. The Dockerfile can exist anywhere, as long as a fully-qualified URL is available.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: All the &lt;code&gt;oc&lt;/code&gt; commands in this article also work with &lt;code&gt;kubectl&lt;/code&gt;, whether you are working with an OpenShift cluster or a Kubernetes cluster, or without a cluster.&lt;/p&gt; &lt;h2&gt;Setup and configuration for kaniko on OpenShift&lt;/h2&gt; &lt;p&gt;Once the prerequisites are set up, configured, and active, you can perform a kaniko build on an OpenShift cluster and push the image to a registry.&lt;/p&gt; &lt;h3&gt;Log in to the OpenShift cluster&lt;/h3&gt; &lt;p&gt;To start, log in to your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login --token=token --server=server-url&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a new project&lt;/h3&gt; &lt;p&gt;Create your own project using:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project project-name&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a secret using the credentials to your registry&lt;/h3&gt; &lt;p&gt;To push your image to an external registry (such as Docker Hub), create a secret named &lt;code&gt;regcred&lt;/code&gt; using the following &lt;code&gt;oc&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret docker-registry regcred \ --docker-server=your-registry-server \ --docker-username=your-name \ --docker-password=your-pword \ --docker-email=your-email&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace the italicized values in this command with the following information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;em&gt;your-registry-server&lt;/em&gt;&lt;/code&gt;: the fully qualified domain name (FQDN) of your private Docker registry (https://index.docker.io/v1/ for Docker Hub)&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-name&lt;/code&gt;&lt;/em&gt;: your Docker username&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-pword&lt;/code&gt;&lt;/em&gt;: your Docker password&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-email&lt;/code&gt;&lt;/em&gt;: your Docker email address&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Push your image to an internal registry through a pod on a cluster using a service account. For example, you can acquire login credentials for your service account, such as for Builder, through the cluster's console.&lt;/p&gt; &lt;p&gt;From the list of available secrets in your namespace, pick a &lt;code&gt;builder-dockercfg&lt;/code&gt; secret, and expose the base64 credentials using the &lt;strong&gt;Reveal Values&lt;/strong&gt; button on the OpenShift console.&lt;/p&gt; &lt;p&gt;Locate the URL for your target image registry and copy the authorization token. Use it to prepare a new &lt;code&gt;config.json&lt;/code&gt; file by replacing&lt;em&gt; image-registry-url &lt;/em&gt;and &lt;em&gt;auth-token&lt;/em&gt; with the appropriate values. For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "auths": { "image-registry-url": { "auth": "auth-token" } }, "HttpHeaders": { "User-Agent": "Docker-Client/19.03.8 (darwin)" }, "experimental": "disabled" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the &lt;code&gt;config.json&lt;/code&gt; file is ready, create a secret as follows, naming it &lt;code&gt;regcred&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic regcred --from-file=.dockerconfigjson=path/to/config.json --type=kubernetes.io/dockerconfigjson&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Clone a source code repository&lt;/h3&gt; &lt;p&gt;In the local file system, &lt;code&gt;git clone&lt;/code&gt; your source code repository. For example, in an empty directory enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git://github.com/openshift/golang-ex.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, download the corresponding Dockerfile from its location and place it at the root of this directory, if it doesn't already exist there.&lt;/p&gt; &lt;p&gt;If a specific subdirectory within your cloned repo hosts the code used to build an image (as opposed to the entire cloned directory), place the Dockerfile at the root of that subdirectory. Together the directory containing your source code and Dockerfile now represent your build context.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: When using the Dockerfile present in the mentioned repository, drop the line that says &lt;code&gt;USER nobody&lt;/code&gt;, to avoid permission issues.&lt;/p&gt; &lt;p&gt;Make sure that the paths mentioned after &lt;code&gt;/kaniko/build-context&lt;/code&gt; against the &lt;code&gt;--dockerfile&lt;/code&gt; and &lt;code&gt;--context&lt;/code&gt; parameters in the &lt;code&gt;openshift-pod.yaml&lt;/code&gt; file accurately represents the directory structure present inside &lt;code&gt;kaniko-build-context.tar.gz&lt;/code&gt;. The paths must be an exact match.&lt;/p&gt; &lt;h3&gt;Compress the build context into a tar.gz file&lt;/h3&gt; &lt;p&gt;Once the build context is ready, compress it into a &lt;code&gt;tar.gz&lt;/code&gt; file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tar -czvf kaniko-build-context.tar.gz path/to/folder&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create an openshift-pod.yaml file with two containers&lt;/h3&gt; &lt;p&gt;Create an &lt;code&gt;openshift-pod.yaml&lt;/code&gt; file that has two containers, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/kaniko-blog.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/kaniko-blog.png?itok=7ezxtVYs" width="600" height="628" title="kaniko-blog" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: An openshift-pod.yaml file with two containers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;If you are pushing to Docker Hub, you could set the destination as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--destination=docker.io/your-dockerhub-username/image-name:image-tag&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are pushing to the internal registry, set the destination as shown here:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--destination=image-registry.openshift-image-registry.svc:5000/your-project-name/image-name:image-tag&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Apply the pod to the cluster&lt;/h3&gt; &lt;p&gt;Use the following command to apply the pod to your cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f path/to/openshift-pod.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command should return:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;pod/kaniko created&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Check the status of the cluster&lt;/h3&gt; &lt;p&gt;To check the cluster's status, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is an example of what it displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;NAME READY STATUS RESTARTS AGE kaniko 0/1 Init:0/1 0 50s&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Copy tar.gz from the local file system to the kaniko-init container&lt;/h3&gt; &lt;p&gt;Copy the &lt;code&gt;tar.gz&lt;/code&gt; file that you created earlier from the local file system to the &lt;code&gt;kaniko-init&lt;/code&gt; container currently running in the pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc cp path/to/kaniko-build-context.tar.gz kaniko:/tmp/kaniko-build-context.tar.gz -c kaniko-init&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Extract the copied tar.gz file on the mounted path to the shared volume&lt;/h3&gt; &lt;p&gt;From inside your &lt;code&gt;kaniko-init&lt;/code&gt; container, extract the copied &lt;code&gt;tar.gz&lt;/code&gt; file into the mounted path pointing to the shared volume inside of the kaniko pod. This allows it to be accessed by other containers with access to this shared volume.&lt;/p&gt; &lt;h2&gt;Check your work&lt;/h2&gt; &lt;p&gt;You should see the pushed image reflected in your target registry. Additionally, you can take a closer look inside the container at any time. (I found this to be quite useful while attempting to debug the process.) To begin, start a bash session inside your &lt;code&gt;kaniko-init&lt;/code&gt; container and take a look:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc exec kaniko -c kaniko-init -it /bin/bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the extraction process is complete, you can shut down the init container, at which point the kaniko container takes over. Then create a file that serves as a trigger:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc exec kaniko -c kaniko-init -- touch /tmp/complete&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you run &lt;code&gt;oc get pods&lt;/code&gt; again, the output displays whether everything is working well:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;NAME READY STATUS RESTARTS AGE kaniko 1/1 Running 0 6m57s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, run the following &lt;code&gt;oc&lt;/code&gt; command to get a more detailed look at what's going on inside the kaniko container:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc describe pod kaniko&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can look at the pod logs within the OpenShift console.&lt;/p&gt; &lt;p&gt;After the pod has reached a completed state, if you pushed it to an external registry, you should be able to log into your registry and find the newly pushed image there. If you pushed to the internal registry, you should be able to navigate to &lt;strong&gt;Builds —&gt;&lt;/strong&gt; &lt;strong&gt;ImageStreams &lt;/strong&gt;(within the OpenShift console's Administrator view) to find the newly pushed image there.&lt;/p&gt; &lt;p&gt;You can delete the pod if needed using &lt;code&gt;oc delete pod kaniko&lt;/code&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/18/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry" title="Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry"&gt;Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w5sV_LpWTN0" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jaideep Rao</dc:creator><dc:date>2021-06-18T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/18/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 18 June 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yUc39IMGP5o/weekly-2021-06-18.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="vert.x" /><category term="java" /><category term="narayana" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-06-18.html</id><updated>2021-06-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, vert.x, java, narayana"&gt; &lt;h1&gt;This Week in JBoss - 18 June 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Congrats to all the teams on their hard work!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/06/keycloak-1400-released"&gt;Keycloak 14.0.0&lt;/a&gt; is released! This release adds Financial-grade API (FAPI) support and lots of improvements.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2021/06/byteman-4016-has-been-released.html"&gt;Byteman 4.0.16&lt;/a&gt; has shipped and is now the latest version if you’re running Java version 9 to 17.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/06/17/WildFly24-Final-Released/"&gt;WildFly 24&lt;/a&gt; is here and brings lots of awesome work. There’s too much to sum up here so click the link and check out the release notes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-0-cr3-released/"&gt;Quarkus 2.0.0.CR3&lt;/a&gt; is here. Towards 2.0 final!!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-1-13-7-final-released/"&gt;Quarkus 1.13.7.Final&lt;/a&gt; is available as a maintenance release.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-0/"&gt;Eclipse Vert.x 4.1.0&lt;/a&gt; is available with lots of exciting features. Go download and start using it!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-3-9-8/"&gt;Eclipse Vert.x 3.9.8&lt;/a&gt; is also here with numerous bug fixes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://jbossts.blogspot.com/2021/06/narayana-5120final-released.html"&gt;Narayana 5.12.0.Final&lt;/a&gt; has been shipped. This version gives several enhancements and fixes some bugs. Go grab it!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Katia and Ryan on the Infinispan team released a nice two-part series of posts that explained how they built an online game across data centers and multiple cloud providers for this year’s Summit keynote demonstration. In the first part, &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;Building a real-time leaderboard with Infinispan and Quarkus on a hybrid Kubernetes deployment&lt;/a&gt;, Katia and Ryan describe how they designed and implemented various services using Infinispan and Quarkus. With part two, they explain how using the Infinispan Operator greatly reduced the complexity of standing up their clusters, &lt;a href="https://developers.redhat.com/articles/2021/06/08/create-and-manage-red-hat-data-grid-services-hybrid-cloud"&gt;Creating and managing Infinispan services in the hybrid cloud&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Tommaso Teofili’s recent post, &lt;a href="https://blog.kie.org/2021/06/autotuning-lime-explanations-with-few-predictions.html"&gt;Autotuning LIME explanations with few predictions&lt;/a&gt;, outlines how to automatically tune LIME hyperparameters to achieve more stable explanations and comes with a PR that lets you dig into all the technical aspects covered in the post.&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/06/rhpam-connectivity-to-external-amq-configurations-in-openshift.html"&gt;RHPAM connectivity to external AMQ configurations in OpenShift&lt;/a&gt; by Michael Perez takes an in-depth look at connecting to an external AMQ with the aim of lowering the memory footprint for RHPAM pods as well as other deployment optimizations. It’s an interesting read with good technical considerations to sink your teeth into so go give it a look.&lt;/p&gt; &lt;p&gt;Over on the WildFly blog, Jeff Mesnil walks us through the process of changing logging levels for cloud-based WildFly applications on the fly. Take a look at Jeff’s script and straightforward commands to help you easily modify logs for debugging in his post, &lt;a href="https://www.wildfly.org/news/2021/06/15/change-log-level-wildfly-cloud/"&gt;How to Change Logging Level for WildFly on the Cloud&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Bilgin Ibryam is back with a post on the Red Hat Developer blog titled &lt;a href="https://developers.redhat.com/articles/2021/06/14/application-modernization-patterns-apache-kafka-debezium-and-kubernetes"&gt;Application modernization patterns with Apache Kafka, Debezium, and Kubernetes&lt;/a&gt;. I really enjoyed this one as Bilgin expertly puts application modernization in context and examines different patterns, tools, and open-source ecosystems that can help you migrate brown-field systems to more modern, event-driven services as well as design green-field services that are future proof by providing the ability to evolve over time. Be sure to catch up on this one if you haven’t already read it.&lt;/p&gt; &lt;p&gt;Just in time for your summer reading list the first book dedicated to Keycloak has been published, &lt;a href="https://www.keycloak.org/2021/06/book.adoc"&gt;Keycloak - Identity and Access Management for Modern Applications&lt;/a&gt;. Congrats to authors Stian Thorgersen and Pedro Igor Silva. It’s an impressive achievement and no doubt the book is full of invaluable expertise.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_evangelists_corner"&gt;Evangelist’s corner&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;In our last editorial we mentioned Eric Schabell’s series on a retail data framework. Since then Eric has obviously been hard at work as his next two posts in the series, &lt;a href="https://www.schabell.org/2021/05/retail-data-framework-common-architectural-elements.html"&gt;Retail data framework - Common architectural elements&lt;/a&gt; and &lt;a href="https://www.schabell.org/2021/06/retail-data-framework-example-data-architecture.html"&gt;Retail data framework - Example data architecture&lt;/a&gt;. If you haven’t caught up on that series yet, then I highly recommend taking a look to learn about data flows and management in a retail context.&lt;/p&gt; &lt;p&gt;Eric has also posted the first in his next series, &lt;a href="https://www.schabell.org/2021/05/cloud-adoption-an-architectural-introduction.html"&gt;Cloud adoption - An architectural introduction&lt;/a&gt;. This series is focused on proven integrations, structures, and interactions with the goal of enabling readers to implement and adopt cloud-based solutions using open-source technologies. I’m really excited about this series and can’t wait to see what Eric brings in his next posts.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some videos from our community. Here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ETTMBWEBfLY"&gt;Quarkus Insights #51: Answering questions from the community&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/pYhaZYX0kq4"&gt;Quarkus Insights #53: Java Memory - why should you care&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ngXii5sA_nA"&gt;Building Kubernetes Native Java with the Quarkus CLI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yUc39IMGP5o" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-06-18.html</feedburner:origLink></entry><entry><title>How to deliver decision services with Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iMnvBPVvqYA/how-deliver-decision-services-kogito" /><author><name>Karina Varela</name></author><id>66d60789-4d39-441c-9382-40faea7db139</id><updated>2021-06-17T07:00:00Z</updated><published>2021-06-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the first of two presenting new support for developing decision services in &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Business Automation Manager&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt;. We specifically address support for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_decision_manager/7.1/html/designing_a_decision_service_using_dmn_models/dmn-support-con_dmn-models"&gt;Decision Model and Notation&lt;/a&gt; (DMN) standard. Process Automation Manager now supports Kogito's cloud-native runtime engine for creating rules, decisions, and resource-planning optimization solutions based on the &lt;a href="http://dmg.org/pmml/pmml-v4-4.html"&gt;Predictive Model Markup Language&lt;/a&gt; (PMML).&lt;/p&gt; &lt;p&gt;We'll present an example using &lt;a href="https://kogito.kie.org/"&gt;Kogito&lt;/a&gt; with &lt;a href="https://drools.org/"&gt;Drools Rules Language&lt;/a&gt;, both backed by the &lt;a href="http://kie.org/"&gt;KIE group&lt;/a&gt;. By expanding Kogito with the power of &lt;a href="https://quarkus.io/"&gt;Quarkus&lt;/a&gt;, you can enjoy hot-reload during the development phase and compile decision services into fast, lightweight services.&lt;/p&gt; &lt;p&gt;For resource planning, Process Automation Manager 7.11 brings full support for &lt;a href="https://www.optaplanner.org/download/releaseNotes/releaseNotes8.html"&gt;OptaPlanner 8&lt;/a&gt;, the most recent version of this &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt; (AI) constraint solver technology.&lt;/p&gt; &lt;p&gt;All these new features are now part of the &lt;a href="https://www.redhat.com/en/products/process-automation"&gt;Red Hat Process Automation&lt;/a&gt; stack. &lt;/p&gt; &lt;h2&gt;Decision services with Kogito&lt;/h2&gt; &lt;p&gt;Kogito is the open source project that brings a new cloud-native runtime engine to &lt;a href="https://developers.redhat.com/products/red-hat-decision-manager/"&gt;Red Hat Decision Manager&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/process-automation-manager"&gt;Process Automation Manager&lt;/a&gt;. Kogito brings Drools and OptaPlanner's battle-tested capabilities with additional improvements for performance and usability. It provides a new set of tools that are better integrated with development IDEs such as &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;Visual Studio Code (VS Code)&lt;/a&gt; and with versioning tools such as GitHub that are already helping developers create decision services more efficiently.&lt;/p&gt; &lt;p&gt;The Kogito decision engine generates 90% of the code you need to get your service up and running. Kogito code-gen uses DMN nodes and data types to infer domain-driven APIs that can be exposed via REST along with the inputs and outputs of the respective APIs, based on the &lt;a href="https://swagger.io/"&gt;Swagger&lt;/a&gt; specification.&lt;/p&gt; &lt;h2&gt;Red Hat support for Kogito tools&lt;/h2&gt; &lt;p&gt;To support the development of decision services, the KIE team provides you with a good set of tooling to use in different spaces. As of today, Red Hat supports the following tools:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Business Central (in Process Automation Manager) or Decision Central (in Decision Manager): A business-friendly user interface.&lt;/li&gt; &lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-extension-red-hat-business-automation-bundle"&gt;Business Automation VS Code Extension&lt;/a&gt;: A VS Code extension that allows the visualization and editing of &lt;a href="https://www.bpmn.org"&gt;Business Process Modeling Notation&lt;/a&gt; (BPMN), DMN, and test scenario files inside VS Code.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Other community-provided tools, free for use, are backed by Red Hat and the KIE team:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://learn-dmn-in-15-minutes.com/"&gt;Learn DMN in 15 minutes&lt;/a&gt;: A guided web-based tour through the elements of Decision Model and Notation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/bpmn-dmn-test-scenario-ed/mgkfehibfkdpjkfjbikpchpcfimepckf"&gt;GitHub Chrome Extension&lt;/a&gt;: A browser extension that allows you to visualize and edit BPMN, DMN, and test scenario files directly in GitHub.&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiegroup.github.io/kogito-online/#/"&gt;Online Editors&lt;/a&gt;: &lt;ul&gt;&lt;li&gt;&lt;a href="http://bpmn.new/"&gt;BPMN.new&lt;/a&gt;: A free online editor for business processes.&lt;/li&gt; &lt;li&gt;&lt;a href="http://dmn.new/"&gt;DMN.new&lt;/a&gt;: A free online editor for decision models.&lt;/li&gt; &lt;li&gt;&lt;a href="http://pmml.new/"&gt;PMML.new&lt;/a&gt;: A free online editor for scorecards.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiegroup.github.io/kogito-online/#/download"&gt;Business Modeler Hub&lt;/a&gt;: Allows downloads of the VS Code Extension, GitHub Chrome extension, and desktop app.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;With the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-extension-red-hat-business-automation-bundle"&gt;VS Code Red Hat Business Automation extension&lt;/a&gt;, you can create new DMN diagrams by simply adding new files with the &lt;code&gt;.dmn&lt;/code&gt; extension.&lt;/p&gt; &lt;h2&gt;Kogito with Decision Model and Notation&lt;/h2&gt; &lt;p&gt;Decision Model and Notation includes a TCK that allows vendors to check the conformity of their tooling. The &lt;a href="https://dmn-tck.github.io/tck/"&gt;DMN TCK&lt;/a&gt; test kit has three different levels of conformance, where Level 3 means that the tested tool provides all the capabilities required by the specification. Currently, the Kogito runtime is compliant with version 1.3, the most recent version of DMN, in conformance with Level 3.&lt;/p&gt; &lt;p&gt;The first Kogito engine version supported by Red Hat is Kogito 1.5.x, which is supported under Red Hat Process Automation version 7.11.x. You can choose to run your decision services on top of the &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Red Hat build of Quarkus&lt;/a&gt; 1.11.x or &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;SpringBoot&lt;/a&gt; 2.3.4.&lt;/p&gt; &lt;h2&gt;Creating a decision service with Kogito and Quarkus&lt;/h2&gt; &lt;p&gt;To create your first decision service, use one of the available Kogito archetypes to generate the project for you. The following command generates a Quarkus-based decision service with the name &lt;code&gt;sample-kogito&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn archetype:generate \ -DarchetypeGroupId=org.kie.kogito \ -DarchetypeArtifactId=kogito-quarkus-archetype \ -DgroupId=org.acme -DartifactId=sample-kogito \ -DarchetypeVersion=1.5.0.Final \ -Dversion=1.0-SNAPSHOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you open the project in VS Code, you can already see a sample DMN created for you, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-05-26%20at%2009.58.15.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screen%20Shot%202021-05-26%20at%2009.58.15.png?itok=vcMXAv1y" width="600" height="413" alt="When you create a project using Kogito, it provides a sample DMN." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Sample DMN created by Kogito. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Notice there are no Java classes or POJOs in this project. You can run this project and check the exposed APIs by running the following command in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the project starts, you should see log messages similar to:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2021-05-26 09:59:49,581 INFO [io.quarkus] (Quarkus Main Thread) sample-kogito 1.0-SNAPSHOT on JVM (powered by Quarkus 1.11.5.Final) started in 4.158s. Listening on: http://localhost:8080 2021-05-26 09:59:49,583 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2021-05-26 09:59:49,583 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy, resteasy-jackson, servlet, smallrye-health, smallrye-openapi, swagger-ui]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the run is successful, you can access &lt;a href="http://http//:localhost:8080/swagger-ui"&gt;http//:localhost:8080/swagger-ui&lt;/a&gt; in your browser and check the existing APIs, as shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kogito-dmn-auto-generated-rest-apis.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kogito-dmn-auto-generated-rest-apis.png?itok=opZtde4d" width="600" height="361" alt="Kogito auto-generates RESTful APIs for the DMN." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: RESTful APIs auto-generated by Kogito for DMN. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Creating and running cloud-native decision microservices is pretty straightforward when using Kogito. If you are interested in trying out Kogito decision services with DMN, here are some places you can go to get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hands-on experience on Katacoda: &lt;a href="https://learn.openshift.com/middleware/courses/middleware-kogito/"&gt;Red Hat OpenShift's Interactive Learning Portal for Kogito&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Kogito Documentation: &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-using-drl-rules"&gt;Using DRL rules in Kogito services&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you are interested in how to run Drools Rules Language-based rules on Kogito, please refer to &lt;a href="https://developers.redhat.com/articles/2021/05/26/delivering-rule-based-services-kogito"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Kogito community is pretty active. You can always reach out to other community members and the Red Hat team behind it through the &lt;a href="http://kie.org/"&gt;community channels&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito" title="How to deliver decision services with Kogito"&gt;How to deliver decision services with Kogito&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iMnvBPVvqYA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Karina Varela</dc:creator><dc:date>2021-06-17T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito</feedburner:origLink></entry><entry><title type="html">Red Hat Summit 2021 (Ask the Experts) - An open approach to solution architectures (video)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YNeLYi8hXA4/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/IThWqUU8sTQ/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html</id><updated>2021-06-17T05:00:00Z</updated><content type="html">This year the  event is a bit different as we bridge the gap from pandemic reality to hopefully a form of normalcy.  As the Red Hat Summit site explains to us, this "...event is expanding to become an all-new, flexible conference series, consisting of a 2‑part immersive virtual experience as well as a global tour of small-scale, in-person events. This series will create collective opportunities to share experiences, innovations, and insights."  and the on-demand recording is available if you missed it.  The event is free, so if you have not yet done so, register and you have full access to all the recordings. Now let's take a look at how to jump straight to our session This year I'm involved with some friends and colleagues in a few Ask The Experts sessions where you get a live session on a topic with several experts available to ask anything you like about the topic. I've done  in April, and I'm going to be involved in the following session next week: Solution architectures are the detailed and structured descriptions of the features, process, and behavior of a solution. It acts as the base of the solution to define, deliver, manage and operate the development process of the solution. It identifies the alternatives of the solutions and its components. In this session, we'll highlight the process that Red Hat uses to compile and publish solution architectures for various business and technology scenarios that are based on actual use cases pertinent to our global customers and partners. It includes the key capabilities of continuous global collaboration with the engineering teams. Date: Tuesday, Jun 15 Time: 16:30 - 17:00 CEST Speakers:  * Eric D. Schabell, Portfolio Architect Technical Director, Red Hat * Will Nix, Senior Manager, Red Hat * E.G Nadhan, Chief Architect and Strategist, Red Hat I hope you enjoyed our discussions, the questions we had time to answer, and the information we shared around how we are developing our open architectures to share with you.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YNeLYi8hXA4" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/IThWqUU8sTQ/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html</feedburner:origLink></entry><entry><title type="html">WildFly 24 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uScxyA4nrxI/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/06/17/WildFly24-Final-Released/</id><updated>2021-06-17T00:00:00Z</updated><content type="html">I’m pleased to announce that the WildFly 24 Final zip is now available . Work during the WildFly 24 development cycle has been primarily oriented toward bug fixing, plus the . We’ve also been doing work on getting WildFly Preview to run well on SE 16 and 17, with a goal of being able to support SE 17 in standard WildFly later this year. There are a number of new features in 24 though: NEW FEATURES * The MicroProfile Reactive Streams Operators subsystem has been updated to support . * It is now possible to strings to verify client-supplied passwords against passwords stored in a Properties Realm, Filesystem Realm, JDBC Realm and LDAP realm in the Elytron subsystem. * Older JDK versions use the protocol SSLv2Hello in the initial handshake message where the SSL version that will be used for the rest of the handshake is negotiated. Although the use of this protocol is discouraged and disabled by default in newer JDK versions, in order to ensure feature parity with legacy security configurations it is now possible to in the Elytron subsystem. * Elytron previously supported configuring one certificate revocation list. However, if several Certificate Authorities were used, there was no way to configure more than one certificate revocation file. It is now possible to in Elytron. * A todo-backend showcases how WildFly can be deployed on OpenShift to provide a backend that exposes an HTTP API (using Jakarta RESTful Web Services) and stores data to a DB (using Jakarta Persistence). CODEHAUS JACKSON REMOVAL Please note that in WildFly 24 we’ve from the server distribution, along with support for the Jakarta RESTful Web Services provider that used Codehaus Jackson. For many years now a provider based on the successor FasterXML Jackson project has been available and is the preferred option for those wanting to use Jackson. WILDFLY PREVIEW As I when we released WildFly 22 Alpha1, along with our traditional Jakarta EE 8 distribution we want to give our users a preview of what will be coming in WildFly as we move on to EE 9 and later. We call this distribution "WildFly Preview". The WildFly 24.0.0.Final release includes an update to WildFly Preview. Even though this is coming from a .Final tag of the WildFly codebase, WildFly Preview should always be regarded as a tech-preview/beta distribution. EE 9 is primarily about implementing the necessary change in the Jakarta EE APIs from the javax.* package namespace to the jakarta.* namespace. This is a big change that is going to take a while to percolate through the EE ecosystem, e.g. for the many projects that compile against the EE APIs to provide versions that use jakarta.*. While this happens we want to continue to deliver new features and fixes to our community, so the primary WildFly distribution will continue to provide the EE 8 APIs. This will continue at least through WildFly 25. STANDARDS SUPPORT The standard WildFly 24.0.0 distribution is a Jakarta EE 8 compatible implementation, compatible with both the Full Platform and the Web Profile. Evidence supporting our certification is available and . Beginning with WildFly 23 we are exclusively focusing on the Jakarta EE test suite for EE certification / compliance. The standard WildFly 24 distribution is also a compliant implementation of the MicroProfile 4.0 platform specification. The WildFly Preview distribution released today is a compatible implementation of both the Jakarta EE 9.1 Web Profile and the Full Platform. Evidence supporting our certification is available and . JDK SUPPORT Our recommendation is that you run WildFly on the most recent long-term support JDK release, i.e. on JDK 11 for WildFly 24. While we do do some testing of WildFly on JDK 13, we do considerably more testing of WildFly itself on the LTS JDKs, and we make no attempt to ensure the projects producing the various libraries we integrate are testing their libraries on anything other than JDK 8 or 11. WildFly 24 also is heavily tested and runs well on Java 8. We plan to continue to support Java 8 at least through WildFly 25, and probably beyond. While we recommend using an LTS JDK release, I do believe WildFly runs well on JDK 13. By run well, I mean the main WildFly testsuite runs with no more than a few failures in areas not expected to be commonly used. We want developers who are trying to evaluate what a newer JVM means for their applications to be able to look to WildFly as a useful development platform. A major focus during the WildFly 24 development cycle was on identifying and addressing issues related to running WildFly on JDK 16 and the early access releases of JDK 17. JDK 17 is due out in September and will be the next LTS JDK release, so we want to be ready to support it as soon as we can. I’m pleased to be able to say that the 24.0.0.Final release of WildFly Preview runs well on SE 16 and the SE 17 early access release for most use cases. The main use case where we still have some issues to resolve relate to Hibernate’s proxy generation, but these don’t prevent JPA from working well in general. For developers wanting to get a sense of what SE 17 will mean for their applications, I encourage you to give WildFly Preview 24 a look. Standard WildFly does not run well on SE 14 or later because the security implementation used in our standard configurations will not work on SE 14 or later. However if you want to experiment with standard WildFly 24 on the latest SE, you can try starting with the configuration files provided in WildFly Preview, which do not use the legacy security subsystem. This has not been heavily tested, so YMMV. Please note that WildFly runs on Java 11 and later in classpath mode. UPCOMING CHANGES Beginning with WildFly 25 there will be some significant changes coming in WildFly, primarily related to moving completely to Elytron-based security and away from the legacy security implementation. Significant changes you can expect to see include: * Removal of the legacy security subsystem from the standard configurations. * Removal of support for the security vault, with Elytron credential stores as the replacement. * Removal of the core management security realms from the standard configurations, with Elytron subsystem resources used instead. * Removal of the Picketlink extension and the subsystems it provides. * Potentially the Picketbox libraries will no longer be available, making the legacy security subsystem and core management security realms unavailable altogether on a server. * Unrelated to security, "legacy feature packs" (which use a provisioning technology that predates Galleon) will no longer be distributed. We’re also evaluating changes in domain mode to limit the set of older version Host Controllers a current Domain Controller must be able to support, reducing the set to some of the more recent WildFly versions. DOCUMENTATION The WildFly 24 documentation is available at the . The WildFly 24 management API documentation is in the . JIRA RELEASE NOTES The full list of issues resolved is available . Issues resolved in the WildFly Core 16 release included with WildFly 24 are available . ENJOY! Thank you for your continued support of WildFly. We’d love to hear your feedback at the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uScxyA4nrxI" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/06/17/WildFly24-Final-Released/</feedburner:origLink></entry><entry><title type="html">Byteman 4.0.16 has been released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/IUHZLNfhkrQ/byteman-4016-has-been-released.html" /><author><name>Andrew Dinn</name></author><id>http://bytemanblog.blogspot.com/2021/06/byteman-4016-has-been-released.html</id><updated>2021-06-16T13:38:00Z</updated><content type="html"> Byteman 4.0.16 is now available from the and from the . It is the latest update release for use on all JDK9+ runtimes up to and including JDK17.   Byteman 4.0.16 removes the JBoss Modules plugin from the release, in the process removing a dependency on the jboss-modules jar. The plugin has never been fully functional on JDK9+ releases. Anyone wishing to use the plugin as is should continue to use earlier releases. Alternatively, you can revert to using the Byteman 3.0.19 which runs on JDK8- releases and includes a fully working version of the plugin.   More details are provided in the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/IUHZLNfhkrQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Andrew Dinn</dc:creator><feedburner:origLink>http://bytemanblog.blogspot.com/2021/06/byteman-4016-has-been-released.html</feedburner:origLink></entry></feed>

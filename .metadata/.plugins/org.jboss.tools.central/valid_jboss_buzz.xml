<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Use source-level annotations to help GCC detect buffer overflows</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/N6L-41fiqsU/use-source-level-annotations-help-gcc-detect-buffer-overflows" /><author><name>Martin Sebor</name></author><id>043223b9-3345-424e-9652-5cd64d10d9af</id><updated>2021-06-25T07:00:00Z</updated><published>2021-06-25T07:00:00Z</published><summary type="html">&lt;p&gt;Out-of-bounds memory accesses such as buffer overflow bugs remain among the most dangerous software weaknesses in 2021 (see &lt;a href="https://cwe.mitre.org/top25/archive/2020/2020_cwe_top25.html"&gt;&lt;em&gt;2020 CWE Top 25 Most Dangerous Software Weaknesses&lt;/em&gt;&lt;/a&gt;). In fact, out-of-bounds write (&lt;a href="https://cwe.mitre.org/data/definitions/787.html"&gt;CWE-787&lt;/a&gt;) jumped from the twelfth position in 2019 to second in 2020, while out-of-bounds read (&lt;a href="https://cwe.mitre.org/data/definitions/125.html"&gt;CWE-125&lt;/a&gt;) moved from the fifth to the fourth position.&lt;/p&gt; &lt;p&gt;Recognizing the importance of detecting coding bugs early in the development cycle, recent GNU Compiler Collection (GCC) releases have significantly improved the compiler's ability to diagnose these dangerous bugs by using warnings such as &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Warray-bounds"&gt;&lt;code&gt;-Warray-bounds&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wformat-overflow"&gt;&lt;code&gt;-Wformat-overflow&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wstringop-overflow"&gt;&lt;code&gt;-Wstringop-overflow&lt;/code&gt;&lt;/a&gt;, and (most recently in GCC 11) &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wstringop-overread"&gt;&lt;code&gt;-Wstringop-overread&lt;/code&gt;&lt;/a&gt;. However, a common limitation shared by all these warnings is that they can only analyze code in a single function at a time. With the exception of calls to a small set of intrinsic functions like &lt;code&gt;memcpy()&lt;/code&gt; built into the compiler, the warnings stop at the function call boundary. That means that when a buffer allocated in one function overflows in a function called from it, the problem is not detected unless the called function is inlined into the caller.&lt;/p&gt; &lt;p&gt;This article describes three kinds of simple source-level annotations that programs can use to help GCC detect out-of-bounds accesses across function call boundaries, even if the functions are defined in different source files:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Attribute &lt;code&gt;access&lt;/code&gt; (first introduced in GCC 10, available in both &lt;a href="https://developers.redhat.com/topics/c/"&gt;C and C++&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Variable-length array (VLA) function parameters (&lt;a href="https://gcc.gnu.org/gcc-11/changes.html"&gt;new in GCC 11&lt;/a&gt;, available in C only)&lt;/li&gt; &lt;li&gt;Array function parameters (new in GCC 11, available in C only)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Attribute access&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;&lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html"&gt;access&lt;/a&gt;&lt;/code&gt; function attribute is useful for functions that take a pointer to a buffer as one argument and its size as another. An example might be the POSIX &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/read.html"&gt;&lt;code&gt;read()&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/write.html"&gt;&lt;code&gt;write()&lt;/code&gt;&lt;/a&gt; pair of functions. Besides letting the programmer associate the two parameters, the attribute also specifies how the function accesses the contents buffer. The attribute applies to function declarations and is used both at call sites and when analyzing the definition of a function to detect invalid accesses.&lt;/p&gt; &lt;p&gt;The attribute has the following syntax:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;access&lt;/code&gt; (&lt;code&gt;&lt;em&gt;access-mode&lt;/em&gt;&lt;/code&gt;, &lt;code&gt;&lt;em&gt;ref-index&lt;/em&gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;code&gt;access&lt;/code&gt; (&lt;code&gt;&lt;em&gt;access-mode&lt;/em&gt;&lt;/code&gt;, &lt;code&gt;&lt;em&gt;ref-index&lt;/em&gt;&lt;/code&gt;, &lt;code&gt;&lt;em&gt;size-index&lt;/em&gt;&lt;/code&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;code&gt;&lt;em&gt;ref-index&lt;/em&gt;&lt;/code&gt; and &lt;code&gt;&lt;em&gt;size-index&lt;/em&gt;&lt;/code&gt; denote positional arguments and give the 1-based argument numbers of the buffer and its size, respectively. The buffer argument &lt;code&gt;&lt;em&gt;ref-index&lt;/em&gt;&lt;/code&gt; can be declared as an ordinary object pointer, including &lt;code&gt;void*&lt;/code&gt;, or using the array form (such as &lt;code&gt;T[]&lt;/code&gt; or &lt;code&gt;T[N]&lt;/code&gt;). It need not point to a complete type. The optional &lt;code&gt;&lt;em&gt;size-index&lt;/em&gt;&lt;/code&gt; must refer to an integer argument that specifies the number of elements of the array the function might access. For buffers of incomplete type such as &lt;code&gt;void*&lt;/code&gt;, the size argument is taken to give the number of bytes. When &lt;code&gt;&lt;em&gt;size-index&lt;/em&gt;&lt;/code&gt; is not specified the buffer is assumed to have one element.&lt;/p&gt; &lt;p&gt;&lt;code&gt;&lt;em&gt;access-mode&lt;/em&gt;&lt;/code&gt; describes how the function accesses the buffer. In GCC 11, four modes are recognized:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;read_only&lt;/code&gt; mode indicates that the function reads data from the provided buffer but doesn't write into it. The buffer is expected to be initialized by the caller. The &lt;code&gt;read_only&lt;/code&gt; mode implies a stronger guarantee than the &lt;code&gt;const&lt;/code&gt; qualifier on the buffer because the qualifier can be cast away and the buffer modified in a well-defined program, provided the buffer object itself isn't &lt;code&gt;const&lt;/code&gt;. The parameter to which the &lt;code&gt;read_only&lt;/code&gt; mode is applied may (but need not) be &lt;code&gt;const&lt;/code&gt;-qualified. Declaring a parameter &lt;code&gt;read_only&lt;/code&gt; has the same meaning as declaring one both &lt;code&gt;const&lt;/code&gt; and &lt;code&gt;restrict&lt;/code&gt; in C99 (although GCC 11 doesn't recognize the two as equivalent).&lt;/li&gt; &lt;li&gt;The &lt;code&gt;write_only&lt;/code&gt; mode indicates that the function writes data into the provided buffer but doesn't read from it. The buffer need not be initialized. Attempting to apply the &lt;code&gt;write_only&lt;/code&gt; mode to a &lt;code&gt;const&lt;/code&gt;-qualified parameter causes a warning and the attribute is ignored. This is effectively the default mode for parameters with no associated attribute &lt;code&gt;access&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;read_write&lt;/code&gt; mode indicates that the function both reads and writes data into the buffer. The buffer is expected to be initialized. Attempting to apply the &lt;code&gt;read_write&lt;/code&gt; mode to a &lt;code&gt;const&lt;/code&gt;-qualified parameter causes a warning and the attribute is ignored.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;none&lt;/code&gt; mode means the function doesn't access the buffer at all. The buffer need not be initialized. This mode is new in GCC 11 and is provided for functions that perform argument validation without accessing the data in the buffer.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following example shows how to use the attribute to annotate the POSIX &lt;code&gt;read()&lt;/code&gt; and &lt;code&gt;write()&lt;/code&gt; functions:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;__attribute__ ((access (write_only, 2, 3))) ssize_t read (int fd, void *buf, size_t nbytes); __attribute__ ((access (read_only, 2, 3))) ssize_t write (int fd, const void *buf, size_t nbytes);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because the &lt;code&gt;read()&lt;/code&gt; function stores data in the provided buffer the attributeÂ access mode is &lt;code&gt;write_only&lt;/code&gt;. Similarly, because &lt;code&gt;write()&lt;/code&gt; reads the data from the buffer the access mode is &lt;code&gt;read_only&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;access&lt;/code&gt; attribute serves a similar function as declaring a function parameter using the variable-length array notation, except it's more flexible. Besides the access mode, the &lt;code&gt;&lt;em&gt;size-index&lt;/em&gt;&lt;/code&gt; argument can associate a pointer with a size that comes after it in the function argument list, as is often the case. We'll discuss the VLA notation in the next section.&lt;/p&gt; &lt;h2&gt;VLA function parameters&lt;/h2&gt; &lt;p&gt;In C (but in GCC 11, not in C++), a function parameter declared using the array notation can refer to nonconstant expressions, including prior parameters to the same function, as its bounds. When the bound refers to another function parameter, that parameter's declaration must precede that of the VLA (GCC provides an extension to get around that language limitation; see &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Variable-Length.html"&gt;Arrays of Variable Length&lt;/a&gt; in the GCC manual). When only the most significant bound uses such a bound, it decays to an ordinary pointer just like any other array. Otherwise, it is a VLA. Since the distinction between the two kinds of arrays in this context is rather subtle, GCC diagnostics refer to both as VLAs. We will follow this simplifying convention as well in the rest of the article. For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;void init_array (int n, int a[n]);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The function takes an ordinary array (or, more precisely, a pointer) as its second argument, whose number of elements is given by the first argument. Although it's not necessarily required by the language, passing the function an array with fewer elements than the first argument indicates is almost certainly a bug. GCC checks calls to such functions and issues warnings when it determines that the array is smaller than expected. For instance, the &lt;a href="https://godbolt.org/z/Tha38rd6f"&gt;&lt;code&gt;vla_init&lt;/code&gt;&lt;/a&gt; program has GCC issue the following warning:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#define N 32 int* f (void) { int *a = (int *)malloc (N); init_array (N, a); return a; }&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; In function '&lt;strong&gt;f&lt;/strong&gt;': &lt;strong&gt;warning&lt;/strong&gt;: '&lt;strong&gt;init_array&lt;/strong&gt;' accessing 128 bytes in a region of size 32 [&lt;strong&gt;-Wstringop-overflow=&lt;/strong&gt;] 10 | &lt;strong&gt;init_array (N, a)&lt;/strong&gt;; | &lt;strong&gt;^~~~~~~~~~~~~~~~~&lt;/strong&gt; &lt;strong&gt;note&lt;/strong&gt;: referencing argument 2 of type '&lt;strong&gt;int *&lt;/strong&gt;' &lt;strong&gt;note&lt;/strong&gt;: in a call to function '&lt;strong&gt;init_array&lt;/strong&gt;' 5 | void &lt;strong&gt;init_array&lt;/strong&gt; (int n, int a[n]); | &lt;strong&gt;^~~~~~~~~~&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;The warning detects the (likely) bug of passing an array to &lt;code&gt;init_array&lt;/code&gt; that's smaller than the first argument indicates.&lt;/p&gt; &lt;p&gt;As already mentioned, a declaration of an array where only the most significant bound is variable doesn't actually declare a VLA, but an ordinary array. Only the less significant bounds matter. What that means is that the declarations in the following &lt;a href="https://godbolt.org/z/xvaeYhnsW"&gt;example&lt;/a&gt; are all valid and equivalent:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;void init_vla (int n, int[n]); void init_vla (int, int[32]); void init_vla (int, int*); void init_vla (int n, int[n + 1]);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That, however, presents a problem: Which of the declarations should be used for the out-of-bounds access warning? The solution implemented in GCC 11 is to trust the first declaration and issue a separate warning, &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wvla-parameter"&gt;&lt;code&gt;-Wvla-parameter&lt;/code&gt;&lt;/a&gt;, for any subsequent redeclarations that suggest a different number of elements in the array. The four declarations in the preceding example then cause the following warnings:&lt;/p&gt; &lt;pre&gt; &lt;strong&gt;warning: &lt;/strong&gt;argument 2 of type '&lt;strong&gt;int[32]&lt;/strong&gt;' declared as an ordinary array [&lt;strong&gt;-Wvla-parameter&lt;/strong&gt;] 2 | void init_vla (int, &lt;strong&gt;int[32]&lt;/strong&gt;); | &lt;strong&gt;^~~~~~~&lt;/strong&gt; &lt;strong&gt;warning: &lt;/strong&gt;argument 2 of type '&lt;strong&gt;int *&lt;/strong&gt;' declared as a pointer [&lt;strong&gt;-Wvla-parameter&lt;/strong&gt;] 3 | void init_vla (int, &lt;strong&gt;int*&lt;/strong&gt;); | &lt;strong&gt;^~~~&lt;/strong&gt; &lt;strong&gt;warning: &lt;/strong&gt;argument 2 of type '&lt;strong&gt;int[n + 1]&lt;/strong&gt;' declared with mismatched bound [&lt;strong&gt;-Wvla-parameter&lt;/strong&gt;] 4 | void init_vla (int, &lt;strong&gt;int[n + 1]&lt;/strong&gt;); | &lt;strong&gt;^~~~~~~~~&lt;/strong&gt; &lt;strong&gt;note: &lt;/strong&gt;previously declared as a variable length array '&lt;strong&gt;int[n]&lt;/strong&gt;' 1 | void init_vla (int n, &lt;strong&gt;int[n]&lt;/strong&gt;); | &lt;strong&gt;^~~~~~&lt;/strong&gt;&lt;/pre&gt; &lt;h2&gt;Array function parameters&lt;/h2&gt; &lt;p&gt;Because of concerns of unbounded stack allocation, VLAs tend to be underused in modern C code, even in contexts like function declarations where they are not only safe but help improve the ability to analyze code. In the absence of VLAs, some projects use a simpler convention to declare function parameters that expect callers to provide access to some constant minimum number of elements, say N, using the ordinary array notation &lt;code&gt;T[N]&lt;/code&gt;. For example, the C standard function &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/tmpnam.html"&gt;&lt;code&gt;tmpnam()&lt;/code&gt;&lt;/a&gt; expects its argument to point to an array with at least &lt;code&gt;L_tmpnam&lt;/code&gt; elements. To make that explicit, GNU libc 2.34 declares it as:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;char *tmpnam (char[L_tmpnam]);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GCC 11 recognizes this convention, and when it determines that a call to the function provides a smaller array, it issues a warning. For example, on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; where &lt;code&gt;L_tmpnam&lt;/code&gt; is defined to 20, for the function shown next GCC issues the following warning:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;void g (void) { char a[16]; if (tmpnam (a)) puts (a); }&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; In function 'g': &lt;strong&gt;warning&lt;/strong&gt;: '&lt;strong&gt;tmpnam&lt;/strong&gt;' accessing 20 bytes in a region of size 16 [-Wstringop-overflow=] 10 | if (&lt;strong&gt;tmpnam (a)&lt;/strong&gt;) | &lt;strong&gt;^~~~~~~~~~&lt;/strong&gt; &lt;strong&gt;note&lt;/strong&gt;: referencing argument 1 of type '&lt;strong&gt;char *&lt;/strong&gt;' &lt;strong&gt;note&lt;/strong&gt;: in a call to function '&lt;strong&gt;tmpnam&lt;/strong&gt;' 3 | extern char* &lt;strong&gt;tmpnam&lt;/strong&gt; (char[L_tmpnam]); | &lt;strong&gt;^~~~~~&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;In addition to function calls, GCC 11 also checks the definitions of functions declared with array parameters and issues warnings for accesses that are out of bounds given the constant bounds. For instance, this definition of the &lt;code&gt;init_array()&lt;/code&gt; function triggers a &lt;code&gt;-Warray-bounds&lt;/code&gt; warning as shown in the Compiler Explorer &lt;a href="https://godbolt.org/z/Ybnc8T7q1"&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;void init_array (int, int a[32]) { a[32] = 0; }&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; In function '&lt;strong&gt;init_array&lt;/strong&gt;': &lt;strong&gt;warning&lt;/strong&gt;: array subscript 32 is outside array bounds of '&lt;strong&gt;int[32]&lt;/strong&gt;' [&lt;strong&gt;-Warray-bounds&lt;/strong&gt;] 3 | &lt;strong&gt;a[32]&lt;/strong&gt; = 0; | &lt;strong&gt;~^~~~&lt;/strong&gt; &lt;strong&gt;note&lt;/strong&gt;: while referencing '&lt;strong&gt;a&lt;/strong&gt;' 1 | void init_array (int, &lt;strong&gt;int a[32]&lt;/strong&gt;) | ~~~~^~~~~&lt;/pre&gt; &lt;p&gt;Similarly to the function redeclarations involving VLA parameters, GCC also checks those involving the array forms of parameters and issues a &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Warray-parameter"&gt;&lt;code&gt;-Warray-parameter&lt;/code&gt;&lt;/a&gt; warning for mismatches as shown in the following &lt;a href="https://godbolt.org/z/o97Pxrrxb"&gt;example&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;void init_array (int, int[32]); void init_array (int, int[16]); void init_array (int n, int[]); void init_array (int n, int*);&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;strong&gt;warning&lt;/strong&gt;: argument 2 of type '&lt;strong&gt;int[16]&lt;/strong&gt;' with mismatched bound [&lt;strong&gt;-Warray-parameter=&lt;/strong&gt;] 2 | void init_array (int, &lt;strong&gt;int[16]&lt;/strong&gt;); | &lt;strong&gt;^~~~~~~&lt;/strong&gt; &lt;strong&gt;warning&lt;/strong&gt;: argument 2 of type '&lt;strong&gt;int[]&lt;/strong&gt;' with mismatched bound [&lt;strong&gt;-Warray-parameter=&lt;/strong&gt;] 3 | void init_array (int n, &lt;strong&gt;int[]&lt;/strong&gt;); | &lt;strong&gt;^~~~~&lt;/strong&gt; &lt;strong&gt;warning&lt;/strong&gt;: argument 2 of type '&lt;strong&gt;int *'&lt;/strong&gt; declared as a pointer [&lt;strong&gt;-Warray-parameter=&lt;/strong&gt;] 4 | void init_array (int n, &lt;strong&gt;int*&lt;/strong&gt;); | &lt;strong&gt;^~~~&lt;/strong&gt; &lt;strong&gt;note&lt;/strong&gt;: previously declared as an array '&lt;strong&gt;int[32]&lt;/strong&gt;' 1 | void init_array (int, &lt;strong&gt;int[32]&lt;/strong&gt;); | &lt;strong&gt;^~~~~~~&lt;/strong&gt;&lt;/pre&gt; &lt;h2&gt;Caveats and limitations&lt;/h2&gt; &lt;p&gt;The features discussed here are unique in one interesting respect: They involve both simple lexical analysis and more involved, flow-sensitive analysis. In theory, lexical warnings can be both sound and complete (that is, they suffer from neither false positives nor false negatives). Because they are handled during lexical analysis, the &lt;code&gt;-Warray-parameter&lt;/code&gt; and &lt;code&gt;-Wvla-parameters&lt;/code&gt; warnings are virtually free of such problems. Flow-based warnings, on the other hand, are inherently neither sound nor complete; rather, they are unavoidably prone to both false positives and negatives.&lt;/p&gt; &lt;h3&gt;False negatives&lt;/h3&gt; &lt;p&gt;To use the &lt;code&gt;access&lt;/code&gt; attributes and detect out-of-bounds accesses, the functions to which they apply must not be inlined. Once a function is inlined into its caller, most of its attributes are usually lost. That can prevent GCC from detecting bugs if the out-of-bounds access cannot easily be determined from the inlined function body. For example, the &lt;code&gt;genfname()&lt;/code&gt; function in the following code listing uses &lt;code&gt;getpid()&lt;/code&gt; to generate a temporary file name in the &lt;code&gt;/tmp&lt;/code&gt; directory. Because on most systems the POSIX &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/getpid.html"&gt;&lt;code&gt;gepid()&lt;/code&gt;&lt;/a&gt; function returns a 32-bit int, the longest name the function can generate is 26 characters (10 for &lt;code&gt;INT_MAX&lt;/code&gt;, plus 16 for the &lt;code&gt;/tmp/tmpfile.txt&lt;/code&gt; string, plus 1 byte for the terminating nul character). When the &lt;code&gt;genfname(a)&lt;/code&gt; call in &lt;code&gt;main()&lt;/code&gt; is not inlined, GCC issues the following warning as expected. But when the call is inlined, the warning disappears. You can see the two scenarios side by side &lt;a href="https://godbolt.org/z/Pv6qbzd1o"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;unistd.h&gt; inline void genfname (char name[27]) { snprintf (name, 27, "/tmp/tmpfile%u.txt", getpid ()); } int main (void) { char name[16]; genfname (name); puts (name); } &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; In function '&lt;strong&gt;main&lt;/strong&gt;': &lt;strong&gt;warning&lt;/strong&gt;: '&lt;strong&gt;f&lt;/strong&gt;' accessing 27 bytes in a region of size 16 [&lt;strong&gt;-Wstringop-overflow=&lt;/strong&gt;] 11 | &lt;strong&gt;f (a)&lt;/strong&gt;; | &lt;strong&gt;^~~~~&lt;/strong&gt; &lt;strong&gt;note&lt;/strong&gt;: referencing argument 1 of type '&lt;strong&gt;char *&lt;/strong&gt;' &lt;strong&gt;note&lt;/strong&gt;: in a call to function '&lt;strong&gt;f&lt;/strong&gt;' 3 | inline void &lt;strong&gt;f&lt;/strong&gt; (char a[27]) | &lt;strong&gt;^&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;As an aside, if you're wondering why the &lt;code&gt;sprintf()&lt;/code&gt; call isn't diagnosed by &lt;code&gt;-Wformat-truncation&lt;/code&gt;, it's because the warning is unable to determine anything about the &lt;code&gt;getpid()&lt;/code&gt; result.&lt;/p&gt; &lt;h3&gt;False positives&lt;/h3&gt; &lt;p&gt;Generally, the detection of out-of-bounds accesses based on the annotations discussed here is subject to the same limitations and shortcomings as all flow-sensitive warnings in GCC. For a detailed discussion of these, see &lt;a href="https://developers.redhat.com/blog/2019/03/13/understanding-gcc-warnings-part-2"&gt;&lt;em&gt;Understanding GCC warnings, Part 2&lt;/em&gt;&lt;/a&gt;. A couple of commonly reported issues specific to the function annotation mechanisms might be worth going over.&lt;/p&gt; &lt;p&gt;As mentioned previously, some projects use the array parameter notation with a constant bound to provide a visual clue that the caller should supply an array with at least as many elements. But sometimes the convention is fuzzy, meaning that the function only uses the array when another parameter has this or that value. Since there is nothing to communicate this "quirk" of the convention to GCC, a warning might end up issued even when the use is safe. We suggest avoiding using the convention in those cases.&lt;/p&gt; &lt;h2&gt;Future work&lt;/h2&gt; &lt;p&gt;In GCC 11, you can use the &lt;code&gt;access&lt;/code&gt; attribute to detect the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Out-of-bounds accesses: &lt;code&gt;-Warray-bounds&lt;/code&gt;, &lt;code&gt;-Wformat-overflow&lt;/code&gt;, &lt;code&gt;-Wstringop-overflow&lt;/code&gt;, and &lt;code&gt;-Wstringop-overread&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Overlapping accesses: &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wrestrict"&gt;&lt;code&gt;-Wrestrict&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Uninitialized accesses: &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wuninitialized"&gt;&lt;code&gt;-Wuninitialized&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the future, we would like to use the attribute to also detect variables that are only written to but never read from (&lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wunused-but-set-parameter"&gt;&lt;code&gt;-Wunused-but-set-parameter&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://gcc.gnu.org/onlinedocs/gcc-11.1.0/gcc/Warning-Options.html#index-Wunused-but-set-variable"&gt;&lt;code&gt;-Wunused-but-set-variable&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;We are also considering extending the &lt;code&gt;access&lt;/code&gt; attribute in some form to function return values as well as to variables. Annotating function return values will let GCC detect attempts to modify immutable objects via pointers returned from functions like &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/getenv.html"&gt;&lt;code&gt;getenv()&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/localeconv.html"&gt;&lt;code&gt;localeconv()&lt;/code&gt;&lt;/a&gt;. Similarly, annotating global variables will make it possible to detect accidentally modifying the contents of objects such as the environment pointer array &lt;a href="https://pubs.opengroup.org/onlinepubs/9699919799/functions/environ.html"&gt;&lt;code&gt;environ&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/25/use-source-level-annotations-help-gcc-detect-buffer-overflows" title="Use source-level annotations to help GCC detect buffer overflows"&gt;Use source-level annotations to help GCC detect buffer overflows&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/N6L-41fiqsU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Martin Sebor</dc:creator><dc:date>2021-06-25T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/25/use-source-level-annotations-help-gcc-detect-buffer-overflows</feedburner:origLink></entry><entry><title type="html">How Quarkus has been used to deploy applications on OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/46RJ-u1Uapw/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-deploys-apps-on-openshift/</id><updated>2021-06-25T00:00:00Z</updated><content type="html">This post gives some feedback on a particular challenge I have been facing in a professional context with respect to deploying applications on Kubernetes, and how we were able to provide a solution that met our goals using Quarkus. The challenge For a few years now I have been involved...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/46RJ-u1Uapw" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-deploys-apps-on-openshift/</feedburner:origLink></entry><entry><title type="html">Custom logic in BPMN</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XF6UrQdTZ0M/custom-logic-in-bpmn.html" /><author><name>Kirill Gaevskii</name></author><id>https://blog.kie.org/2021/06/custom-logic-in-bpmn.html</id><updated>2021-06-24T08:04:08Z</updated><content type="html">There are several ways to add additional custom logic to the Business Process. This article provides a review of different possibilities and their pros and cons. EMBEDDED CODE The easiest way, but not the most comfortable, is to use Script Task and its Script field. When you work with a Script Task, all you need to do is add the node to the process, and you are ready to write Java/JavaScript/MVEL code using the Script property of the task: &gt; NOTE: JavaScript is deprecated and can be removed in the following releases This way of adding custom logic to the process is very fast. Also, there are no performance issues since all scripts are processed just once with Java code and other assets during the project compile phase. However, this way is not very convenient in the long term due to hard maintainability. WHY USE SCRIPT FIELDS * No configuration is needed, just write your code! * As fast as any Java logic * During process model time, there is no need to understand implementation differences between task types DOWNSIDES OF SCRIPT FIELDS * It is not possible to debug this code * Code block has limited syntax highlight and autocompletes features * Itâs hard to track changes using version control systems * The use of fully qualified names (FQNs) to use classes. As an alternative import the class for the whole process * Code can be removed unintentionally by morphing Script Task to any other task type &gt; NOTE: for jBPM there is also an additional option to write quick snippets of &gt; code with the same benefits and downsides as for Script fields: On Entry and &gt; On Exit Actions for different types of activities. This functionality is not &gt; present today in Kogito but it will be available very soon CUSTOM TASK Custom Tasks is a powerful feature. You can predefine the nodeâs different visual and runtime properties on the canvas. An example can be predefined input and output parameters, as well as a custom task icon, task name, documentation, and other task parameters. Also, using Custom Task, you can specify the custom of the task, which can be used to create a unique experience for the new BPMN node. Custom Tasks explained in detail in another article. Below are the pros and cons of this type of activity: WHY USE CUSTOM TASK * Itâs possible to debug Custom Taskâs logic * All code will be edited directly in VS Code with all perks like syntax highlight and autocomplete * Itâs possible to write unit tests for the Taskâs logic * Itâs easy to track changes using version control systems * Itâs possible to predefine different parameters such as Data Input/Outputs * It is possible to predefine some visual parameters as well, like icon and task name * Custom Task can be used as a good template to reuse similar tasks parameters for the whole project DOWNSIDES OF CUSTOM TASK * You need to use a specific task structure and interface, which makes logic a bit harder to reuse between different task types * During the process model, the user should orient in implementation details. Instead of the small number of well-defined tasks (Manual, Automated, Rules) user will see a big amount of different tasks * If you are using different flavors of BPMN Designer like VS Code, browser plugin, standalone editor, desktop version, and so on, you need to take care to support the WID file between all those editorâs versions. * It is not possible to simply share BPMN files between two and more people. To do so, WID files should be shared and correctly used as well. * All task properties configurations Â and configuration files should be located correctly. SERVICE TASK Service Task is a compromise between simplicity of Script Task and features of Custom Task and usually suitable for most of the use cases. Service task usage doesnât need additional knowledge about implementation for Business Analysts, no need for additional files. Also not tied to any interface and donât need any registration as Custom tasks do. &gt; NOTE: In jBPM, Service Tasks canât be placed inside of normal Java application &gt; life cycle which means it is not possible to use features like Java &gt; Annotations or CDI in Service Task if the project will run in jBPM. Thatâs why &gt; Custom Tasks can be preferable for jBPM projects. However, if the project will &gt; run under Kogito all features from the Java life cycle are available and &gt; Service Task can be a preferable choice. HOW TO USE SERVICE TASK ON KOGITO To use the Service task, you will need a Java bean located in your project or project dependencies and mark it with CDI annotation. package com.github.hasys; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class TestService { public String greetUser(String name, Integer age) { System.out.println(String.format("Hello %s with age %o", name, age)); return String.format("User %s greeted.", name); } } To configure Service Task to use this Java bean, its FQN should be used as Interface property and method name as Operation. Assignments are used to send data to and get data from the Service Task.Â The output will always have the name Result for jBPM and can be any for Kogito. Inputs should be the same as the methodâs parameters for both jBPM and Kogito: You can check examples in detail in our repository. WHY USE SERVICE TASK * Itâs possible to debug Custom Taskâs logic * All code will be edited directly in VS Code with all perks like syntax highlight and autocomplete * Itâs possible to write unit tests for the Taskâs logic * Itâs easy to track changes using version control systems * The process can be shared with Business Analysts without additional files and configurations in different flavors of BPMN Editor * During process model time, there is no need to understand implementation differences between tasks * For service tasks, plain Java bean classes used, which makes business logic flexible and reusable DOWNSIDES OF SERVICE TASK * Not possible to predefine different Task parameters * Java bean should be linked for each Service Task separately CONCLUSION There are several possibilities for how to add custom logic to your Process. All of them have their benefits and limitations to fill needs for any use cases. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XF6UrQdTZ0M" height="1" width="1" alt=""/&gt;</content><dc:creator>Kirill Gaevskii</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/custom-logic-in-bpmn.html</feedburner:origLink></entry><entry><title>Automating rule-based services with Java and Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gO2utZKK7QQ/automating-rule-based-services-java-and-kogito" /><author><name>Karina Varela</name></author><id>7ca37207-8e6d-495d-ada6-d782a14638cd</id><updated>2021-06-24T07:00:00Z</updated><published>2021-06-24T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/topics/automation/whats-business-automation"&gt;Business automation&lt;/a&gt; today is a constant and critical task for organizations that seek to formalize policies and ensure that they can be executed, maintained, monitored, and applied during daily operations. This article demonstrates how to use the Kogito engine to automate business rules by implementing them in the Drools Rules Language (DRL).&lt;/p&gt; &lt;p&gt;DRL is common in Drools-based projects. To start using it with Kogito, you need to understand the concept of &lt;em&gt;rule units&lt;/em&gt;. You'll learn how to work with rule units in practice by writing a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; service that automates a piece of business logic with rule units and minimal coding. These capabilities are now part of &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt; 7.11.x, released on June 17.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito"&gt;&lt;em&gt;How to deliver decision services with Kogito&lt;/em&gt;&lt;/a&gt; for more about creating a decision service with Decision Model and Notation (DMN) in Kogito and Quarkus.&lt;/p&gt; &lt;h2&gt;Automating business rules with Kogito&lt;/h2&gt; &lt;p&gt;Bringing all the maturity of Drools, Kogito is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project with a lightweight, fast, and cloud-native runtime engine. Developers using Kogito can expect runtime environment improvements and new a tooling set to use along with IDEs like VS Code. Starting with Process Automation Manager version 7.11.x, it is possible to use a supported version of Kogito based on version 1.5.x . You can choose between running decision services with two runtimes: The &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Red Hat build of Quarkus&lt;/a&gt; 1.11.x or &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; 2.3.4.&lt;/p&gt; &lt;p&gt;In Kogito, you can define business rules using Drools Rules Language or decision tables written in the XLS format. If you are familiar with decision tables, you shouldn't notice many differences when using them with Kogito. To use DRL-based rules, however, you need to get used to the concept of rule units.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;rule unit&lt;/em&gt; aggregates a set of rules along with a description of the working memory that these rules will act upon, also called &lt;em&gt;data sources&lt;/em&gt;. Kogito uses rule units to generate an executable model based on the rules, with a faster startup time. Kogito Codegen also takes advantage of Drools Rules Language queries to discover possible &lt;a href="https://developers.redhat.com/topics/api-management"&gt;APIs&lt;/a&gt; to expose via REST, based on queries defined in the rule unit.&lt;/p&gt; &lt;h2&gt;Using rule units in Kogito&lt;/h2&gt; &lt;p&gt;We'll get started with Kogito and rule units through a simple example that defines a person and whether that person is an adult.&lt;/p&gt; &lt;p&gt;Start with a POJO (plain old Java object) named &lt;code&gt;Person&lt;/code&gt; with the following attributes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme.domain; public class Person { private String name; private int age; private boolean adult; // (getters and setters omitted) } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we can define the rule unit, which we call &lt;code&gt;PersonUnit&lt;/code&gt;. It extends the &lt;code&gt;RuleUnitData&lt;/code&gt; class &lt;code&gt;org.kie.kogito.rules&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; package org.acme; import org.acme.domain.Person; import org.kie.kogito.rules.DataSource; import org.kie.kogito.rules.DataStore; import org.kie.kogito.rules.RuleUnitData; public class PersonUnit implements RuleUnitData { private DataStore&lt;Person&gt; persons = DataSource.createStore(); private int adultAge; public PersonUnit() { } //getters and setters omitted } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Our DRL rule implementation is linked to the rule unit we have just defined by the declaration unit &lt;code&gt;PersonUnit&lt;/code&gt;. To write the &lt;code&gt;Is Adult&lt;/code&gt; rule, we use OOPath, a version of the standard &lt;a href="https://www.w3.org/TR/xpath/"&gt;XPath&lt;/a&gt; language that is designed for use with objects. It provides a succinct and readable way to work with DRL.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; package org.acme; unit PersonUnit; //This links this rule back to our RuleUnit import org.acme.domain.Person; rule "Is Adult" when $p: /persons[age &gt;= 18]; //Tests the age of any Person object in the datastore persons declared in the rule unit then $p.setAdult(true); end query "adult" // Allows Kogito code-gen to expose a domain-driven api to search for adults. $p: /persons; end &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;As this example shows, Kogito helps business automation developers create lightweight rules services quickly with Quarkus or Spring Boot, package the services traditionally or using native compilation, and even deploy them as functions with &lt;a href="https://knative.dev/"&gt;KNative&lt;/a&gt; on top of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. For resource planning, Process Automation Manager fully supports &lt;a href="https://www.optaplanner.org/"&gt;OptaPlanner&lt;/a&gt; version 8, the most recent version of this &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; constraint-solver technology.&lt;/p&gt; &lt;h2&gt;Related tools and support&lt;/h2&gt; &lt;p&gt;If you are interested in trying out Kogito and rule units, check out these places where you can get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The OpenShift interactive &lt;a href="https://learn.openshift.com/middleware/courses/middleware-kogito/rules"&gt;learning portal for Kogito rules&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-using-drl-rules"&gt;Kogito documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The community channels at the &lt;a href="http://kie.org/"&gt;Kogito community&lt;/a&gt;, including its core contributors team.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Also see &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito"&gt;my previous article&lt;/a&gt;, which has more information about using Kogito decision services with Decision Model and Notation (DMN) and creating a decision service with Kogito and Quarkus.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Kogito brings back the joy for business automation developers, who can now quickly develop lightweight rules services, package them traditionally or using native compilation, and even deploy them as functions with &lt;a href="https://developers.redhat.com/books/knative-cookbook"&gt;KNative on top of Kubernetes and OpenShift&lt;/a&gt;. Try it out today, share your feedback, and contribute. The architectural possibilities are endless and flexible to suit on-premises or cloud environments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/24/automating-rule-based-services-java-and-kogito" title="Automating rule-based services with Java and Kogito"&gt;Automating rule-based services with Java and Kogito&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gO2utZKK7QQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Karina Varela</dc:creator><dc:date>2021-06-24T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/24/automating-rule-based-services-java-and-kogito</feedburner:origLink></entry><entry><title>How the JIT compiler boosts Java performance in OpenJDK</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/s8alKH6PfXc/how-jit-compiler-boosts-java-performance-openjdk" /><author><name>Roland Westrelin</name></author><id>36a9d0ce-3c1d-4906-83f9-32df0df7a5c5</id><updated>2021-06-23T07:00:00Z</updated><published>2021-06-23T07:00:00Z</published><summary type="html">&lt;p&gt;Just-in-time (JIT) compilation is central to peak performance in modern virtual machines, but it comes with trade-offs. This article introduces you to JIT compilation in HotSpot, &lt;a href="https://developers.redhat.com/products/openjdk/"&gt;OpenJDK&lt;/a&gt;'s &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; virtual machine. After reading the article, you will have an overview of HotSpot's multi-tiered execution model and how it balances the resources required by your Java applications and by the compiler itself. You'll also see two examples that demonstrate how a JIT compiler uses advanced techniquesâdeoptimization and speculationâto boost application performance.&lt;/p&gt; &lt;h2&gt;How a JIT compiler works&lt;/h2&gt; &lt;p&gt;At its core, a JIT compiler relies on the same well-known compilation techniques that an offline compiler such as the GNU Compiler Collection (GCC) uses. The primary difference is that a just-in-time compiler runs in the same process as the application and competes with the application for resources. As a consequence, a different set of trade-offs in the JIT compiler design applies. Mainly, compilation time is more of an issue for a JIT compiler than for an offline compiler, but new possibilities for optimizationâsuch as deoptimization and speculationâopen up, as well.&lt;/p&gt; &lt;h2&gt;The JIT compiler in OpenJDK&lt;/h2&gt; &lt;p&gt;A Java-based JIT compiler takes &lt;code&gt;.class&lt;/code&gt; files as input rather than Java code, which is consumed by &lt;code&gt;javac&lt;/code&gt;. In this way, a JIT compiler differs from a compiler like GCC, which directly consumes the code that you produce. The JIT compiler's role is to turn class files (composed of &lt;em&gt;bytecode&lt;/em&gt;, which is the JVM's instruction set) into machine code that the CPU executes directly. Another way a Java JIT compiler differs from an offline compiler is that the JVM verifies class files at load time. When it's time to compile, there's little need for parsing or verification.&lt;/p&gt; &lt;p&gt;Other than performance variations, the JIT compiler's execution is transparent to the end user. It can, however, be observed by running the &lt;code&gt;java&lt;/code&gt; command with diagnostic options. As an example, take this simple HelloWorld program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;public class HelloWorld { public static void main(String[] args) { System.out.println("Hello world!"); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's the output from running this program with the version of OpenJDK that happens to be installed on my system:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ java -XX:+PrintCompilation HelloWorld Â Â Â Â Â 50Â  Â  1 Â  Â  Â  3 Â  Â  Â  java.lang.Object::&lt;init&gt; (1 bytes) Â Â Â Â Â 50Â  Â  2 Â  Â  Â  3 Â  Â  Â  java.lang.String::hashCode (55 bytes) Â Â Â Â Â 51Â  Â  3 Â  Â  Â  3 Â  Â  Â  java.lang.String::indexOf (70 bytes) Â Â Â Â Â 51Â  Â  4 Â  Â  Â  3 Â  Â  Â  java.lang.String::charAt (29 bytes) Â Â Â Â Â 51Â  Â  5 Â  Â  n 0 Â  Â  Â  java.lang.System::arraycopy (native) Â  (static) Â Â Â Â Â 52Â  Â  6 Â  Â  Â  3 Â  Â  Â  java.lang.Math::min (11 bytes) Â Â Â Â Â 52Â  Â  7 Â  Â  Â  3 Â  Â  Â  java.lang.String::length (6 bytes) Â Â Â Â Â 52Â  Â  8 Â  Â  Â  3 Â  Â  Â  java.lang.AbstractStringBuilder::ensureCapacityInternal (27 bytes) Â Â Â Â Â 52Â  Â  9 Â  Â  Â  1 Â  Â  Â  java.lang.Object::&lt;init&gt; (1 bytes) Â Â Â Â Â 53Â  Â  1 Â  Â  Â  3 Â  Â  Â  java.lang.Object::&lt;init&gt; (1 bytes) Â  made not entrant Â Â Â Â Â 55 Â  10 Â  Â  Â  3 Â  Â  Â  java.lang.String::equals (81 bytes) Â Â Â Â Â 57 Â  11 Â  Â  Â  1 Â  Â  Â  java.lang.ref.Reference::get (5 bytes) Â Â Â Â Â 58 Â  12 Â  Â  Â  1 Â  Â  Â  java.lang.ThreadLocal::access$400 (5 bytes) Hello world!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Most lines of the diagnostic output list a method that's JIT-compiled. However, a quick scan of the output reveals that neither the &lt;code&gt;main&lt;/code&gt; method nor &lt;code&gt;println&lt;/code&gt; are compiled. Instead, a number of methods not present in the &lt;code&gt;HelloWorld&lt;/code&gt; class were compiled. This brings up a few questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Where do those methods come from, if not from HelloWorld? &lt;/strong&gt;Some code supporting the JVM startup, as well as most of the Java standard library, is implemented in Java, so it is subject to JIT compilation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If none of its methods are compiled, then how does the HelloWorld class run? &lt;/strong&gt;In addition to a JIT compiler, HotSpot embeds an interpreter, which encodes the execution of each JVM bytecode in the most generic way. All methods start executing interpreted, where each bytecode is executed one at a time. Compiled code is tailored to a particular Java method, so optimizations can be applied across bytecode boundaries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why not have the JIT compiler prepare the code for faster results? &lt;/strong&gt;That would require the user code to wait for the JIT'ed code to be ready, which would cause a noticeable pause because compilation takes time.&lt;/p&gt; &lt;h2&gt;HotSpot's JIT execution model&lt;/h2&gt; &lt;p&gt;In practice, the HotSpot JVM's execution model is the result of four observations taken together:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Most code is only executed uncommonly, so getting it compiled would waste resources that the JIT compiler needs.&lt;/li&gt; &lt;li&gt;Only a subset of methods is run frequently.&lt;/li&gt; &lt;li&gt;The interpreter is ready right away to execute any code.&lt;/li&gt; &lt;li&gt;Compiled code is much faster but producing it is resource hungry, and it is only available after the compilation process is over which takes time.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The resulting execution model could be summarized as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Code starts executing interpreted with no delay.&lt;/li&gt; &lt;li&gt;Methods that are found commonly executed (hot) are JIT compiled.&lt;/li&gt; &lt;li&gt;Once compiled code is available, the execution switches to it.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Multi-tiered execution&lt;/h3&gt; &lt;p&gt;In HotSpot, the interpreter instruments the code that it executes; that is, it maintains a per-method count of the number of times a method is entered. Because hot methods usually have loops, it also collects the number of times a branch back to the start of a loop is taken. On method entry, the interpreter adds the two numbers and if the result crosses a threshold, it enqueues the method for compilation. A compiler thread running concurrently with threads executing Java code then processes the compilation request. While compilation is in progress, interpreted execution continues, including for methods in the process of being JIT'ed. Once the compiled code is available, the interpreter branches off to it.&lt;/p&gt; &lt;p&gt;So, the trade-off is roughly between the fast-to-start-but-slow-to-execute interpreter and the slow-to-start-but-fast-to-execute compiled code. How slow-to-start that compiled code is, is under the virtual machine designer's control to some extent: The compiler can be designed to optimize less (in which case code is available sooner but doesn't perform as well) or more (leading to faster code at a later time). A practical design that leverages this observation is to have a multi-tier system.&lt;/p&gt; &lt;h3&gt;The three tiers of execution&lt;/h3&gt; &lt;p&gt;HotSpot has a three-tiered system consisting of the interpreter, the quick compiler, and the optimizing compiler. Each tier represents a different trade-off between the delay of execution and the speed of execution. Java code starts execution in the interpreter. Then, when a method becomes &lt;em&gt;warm&lt;/em&gt;, it's enqueued for compilation by the quick compiler. Execution switches to that compiled code when it's ready. If a method executing in the second tier becomes &lt;em&gt;hot&lt;/em&gt;, then it's enqueued for compilation by the optimizing compiler. Execution continues in the second-tier compiled code until the faster code is available. Code compiled at the second tier has to identify when a method becomes hot, so it also has to increment invocation and back-branch counters.&lt;/p&gt; &lt;p&gt;Note that this description is simplified: The implementation tries to not overwhelm compiler threads with requests, and to balance execution speed with compilation load. As a consequence, thresholds that trigger compilations are not fixed and the second tier is actually split into several sub-tiers.&lt;/p&gt; &lt;p&gt;In HotSpot, for historical reasons, the second tier is known as C1 or the client compiler and the optimizing tier is known as C2, or the server compiler.&lt;/p&gt; &lt;p&gt;In the HelloWorld example, the third column of numbers in the diagnostic output identifies the tier at which code is compiled. Tiers 1 to 3 are subtiers of the low-tier compiler. Tier 4 is the optimizing compiler. As can be seen in the output, that example is so short-lived that no method reaches the optimizing compiler.&lt;/p&gt; &lt;h2&gt;Deoptimization and speculation&lt;/h2&gt; &lt;p&gt;Figure 1 shows a simplified diagram of the state of a method execution.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="This simple method flow moves from left to right." data-entity-type="file" data-entity-uuid="b262fb54-e618-4d55-a387-59b3d8a78a42" src="https://developers.redhat.com/sites/default/files/inline-images/JIT%20in%20hotspot%20blog%20post1_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The state of a method execution moving from left to right.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;State changes happen left to right. This is incomplete as, perhaps surprisingly, state transitions also exist right to left (that is, from more optimized code to less optimized code), as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="State transitions move in a loop: from left to right and from right to left." data-entity-type="file" data-entity-uuid="d45a679b-7ed1-4e62-9073-3ed4408be405" src="https://developers.redhat.com/sites/default/files/inline-images/JIT%20in%20hotspot%20blog%20post2.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: State transitions also move from right to left.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In HotSpot jargon, that process is called &lt;em&gt;deoptimization&lt;/em&gt;. When a thread deoptimizes, it stops executing a compiled method at some point in the method and resumes execution in the same Java method at the exact same point, but in the interpreter.&lt;/p&gt; &lt;p&gt;Why would a thread stop executing compiled code to switch to much slower interpreted code? There are two reasons. First, it is sometimes convenient to not overcomplicate the compiler with support for some feature's uncommon corner case. Rather, when that particular corner case is encountered, the thread deoptimizes and switches to the interpreter.&lt;/p&gt; &lt;p&gt;The second and main reason is that deoptimization allows the JIT compilers to &lt;em&gt;speculate&lt;/em&gt;. When speculating, the compiler makes assumptions that should prove correct given the current state of the virtual machine, and that should let it generate better code. However, the compiler can't prove its assumptions are true. If an assumption is invalidated, then the thread that executes a method that makes the assumption deoptimizes in order to not execute code that's erroneous (being based on wrong assumptions).&lt;/p&gt; &lt;h3&gt;Example 1: Null checks in the C2 tier&lt;/h3&gt; &lt;p&gt;An example of speculation that C2 uses extensively is its handling of null checks. In Java, every field or array access is guarded by a null check. Here is an example in pseudocode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; if (object == null) { Â Â throw new NullPointerException(); } val = object.field;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's very uncommon for a &lt;code&gt;NullPointerException&lt;/code&gt; (NPE) to not be caused by a programming error, so C2 speculatesâthough it cannot prove itâthat NPEs never occur. Here's that speculation in pseudocode:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; if (object == null) { Â Â deoptimize(); } val = object.field;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If NPEs never occur, all the logic for exception creation, throwing, and handling is not needed. What if a null object is seen at the field access in the pseudocode? The thread deoptimizes, a record is made of the failed speculation, and the compiled method's code is dropped. On the next JIT compilation of that same method, C2 will check for a failed speculation record before speculating again that no null object is seen at the field access.&lt;/p&gt; &lt;h3&gt;Example 2: Class hierarchy analysis&lt;/h3&gt; &lt;p&gt;Now, let's look at another example, starting with the following code snippet:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;class C { Â Â void virtualMethod() {} } void compiledMethod(C c) { Â Â c.virtualMethod(); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The call in &lt;code&gt;compiledMethod()&lt;/code&gt; is a virtual call. With only class &lt;code&gt;C&lt;/code&gt; loaded but none of its potential subclasses, that call can only invoke &lt;code&gt;C.virtualMethod()&lt;/code&gt;. When &lt;code&gt;compiledMethod()&lt;/code&gt; is JIT compiled, the compiler could take advantage of that fact to devirtualize the call. But what if, at a later point, a subclass of class &lt;code&gt;C&lt;/code&gt; is loaded? Executing &lt;code&gt;compiledMethod()&lt;/code&gt;, which is compiled under the assumption that &lt;code&gt;C&lt;/code&gt; has no subclass, could then cause an incorrect execution.&lt;/p&gt; &lt;p&gt;The solution to that problem is for the JIT compiler to record a dependency between the compiled method of &lt;code&gt;compiledMethod()&lt;/code&gt; and the fact that &lt;code&gt;C&lt;/code&gt; has no subclass. The compiled method's code itself doesn't embed any extra runtime check and is generated as if &lt;code&gt;C&lt;/code&gt; had no subclass. When a method is loaded, dependencies are checked. If a compiled method with a conflicting dependency is found, that method is marked for deoptimization. If the method is on the call stack of a thread, when execution returns to it, it immediately deoptimizes. The compiled method for &lt;code&gt;compiledMethod()&lt;/code&gt; is also made &lt;em&gt;not entrant&lt;/em&gt;, so no thread can invoke it. It will eventually be reclaimed. A new compiled method can be generated that will take the updated class hierarchy into account. This is known as &lt;em&gt;class hierarchy analysis&lt;/em&gt; (CHA).&lt;/p&gt; &lt;h3&gt;Synchronous and asynchronous deoptimization&lt;/h3&gt; &lt;p&gt;Speculation is a technique that's used extensively in the C2 compiler beyond these two examples. The interpreter and lowest tier actually collect profile data (at subtype checks, branches, method invocation, and so on) that C2 leverages for speculation. Profile data either count the number of occurrences of an eventâthe number of times a method is invoked, for instanceâor collect constants (that is, the type of an object seen at a subtype check).&lt;/p&gt; &lt;p&gt;From these examples, we can see two types of deoptimization events: &lt;em&gt;Synchronous events&lt;/em&gt; are requested by the thread executing compiled code, as we saw in the example of the null check. These events are also called &lt;em&gt;uncommon traps&lt;/em&gt; in HotSpot. &lt;em&gt;Asynchronous events&lt;/em&gt; are requested by another thread, as we saw in the example of the class hierarchy analysis.&lt;/p&gt; &lt;h3&gt;Safepoints and deoptimization&lt;/h3&gt; &lt;p&gt;Methods are compiled so deoptimization is only possible at locations known as &lt;em&gt;safepoints&lt;/em&gt;. Indeed, on deoptimization, the virtual machine has to be able to reconstruct the state of execution so the interpreter can resume the thread at the point in the method where compiled execution stopped. At a safepoint, a mapping exists between elements of the interpreter state (locals, locked monitors, and so on) and their location in compiled codeâsuch as a register, stack, etc.&lt;/p&gt; &lt;p&gt;In the case of a synchronous deoptimization (or uncommon trap), a safepoint is inserted at the point of the trap and captures the state needed for the deoptimization. In the case of an asynchronous deoptimization, the thread in compiled code has to reach one of the safepoints that were compiled in the code in order to deoptimize.&lt;/p&gt; &lt;p&gt;As part of the optimization process, compilers often re-order operations so the resulting code runs faster. Re-ordering operations across a safepoint in that way would cause the state at the safepoint to differ from the state expected at that location by the interpreter and canât be allowed. As a consequence, itâs not feasible to have a safepoint for every bytecode of a method. Doing so would constrain optimizations too much. A compiled method only includes a few safepoints (on return, at calls, and in loops) and a balance is needed between &lt;em&gt;common enough&lt;/em&gt; safepoints, so a deoptimization is not delayed, and &lt;em&gt;rare enough&lt;/em&gt; safepoints, so the compiler has the freedom to optimize between two of them.&lt;/p&gt; &lt;p&gt;This also affects garbage collection and other virtual machine operations that rely on safepoints. Indeed, garbage collection operations need the locations of live objects on a threadâs stacks, which are only available at safepoints. In general, in compiled code, safepoints are the only locations where state that the virtual machine can work with is available.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;To summarize the discussion in this article, a JIT compiler, at its core, is just another compiler. But because it shares resources with the application, the amount of resources used for compilation must be balanced with the benefits of having compiled code. Running concurrently with the application code also has a benefit: It allows compiled code to be tailored to the observed state of the virtual machine through speculation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk" title="How the JIT compiler boosts Java performance in OpenJDK"&gt;How the JIT compiler boosts Java performance in OpenJDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/s8alKH6PfXc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Roland Westrelin</dc:creator><dc:date>2021-06-23T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk</feedburner:origLink></entry><entry><title type="html">Getting started with Red Hat Business Automation version 7.11</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Tts7KfdRalc/getting-started-with-red-hat-business-automation-version-711.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/f44Ha1v5S4U/getting-started-with-red-hat-business-automation-version-711.html</id><updated>2021-06-23T05:00:00Z</updated><content type="html">Â This last week the new release of the Red Hat Business Automation products went live, spanning and with a new version 7.11. These two products provide a lot over versatility to your developer toolbox and there are some getting started documentation and examples to be found in the above linked pages. Over the years I've shared so around all the generations of Red Hat Business Automation products, that you might like to have a little overview of the ones that are now fully updated for use? Below you'll find a walk through the various projects, demos, and workshops available today for you to get started with the latest and greatest of Red Hat Business Automation tools. Let's start with the updates that are designed to show you how to get started with the business automation tooling in a hands-on exploratory fashion. This is a true for building online retail web shop using Red Hat Decision Manager. It's designed to walk you from nothing, to installing the decision manager tooling, to building your project, leveraging a front end project, and deploying it all on your local machine. Click on image to open workshop and view table of contents This project is used in the above workshop to quickly get you up and running with the latest decision manager tooling. In just you're fully installed and ready to start developing your projects. An independent project that with a Vaadin front end and rules project supporting online shopping experience. Get hands-on in just minutes with this complete project. This is a trueÂ Â for building a human resources employee rewards process using Red Hat Process Automation Manager. It's designed to walk you from nothing, to installing the process automation tooling, to building your project, and deploying it all on your local machine. Click on image to open workshop and view table of contents This project is used in the above workshop to quickly get you up and running with the latest process automation tooling. In justÂ Â you're fully installed and ready to start developing your projects. After all of this we have another workshop to get a bit deeper into a real life use case outside of the normal process world. This is anotherÂ Â for building a four eyes principle DevOps process project using Red Hat Process Automation Manager. Click on image to open workshop and view table of contents This project is with human task integration, rule integration, task forms, and features the use of a signal to add a customer contact moment for the marketing department. It installs the process automation tooling and project on your local machine in an automated fashion. Note that all of these projects and workshops are targeting your local machine, in the next article I'll share more content that can be used in containers, operators, and on the OpenShift in a cloud native fashion.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Tts7KfdRalc" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/f44Ha1v5S4U/getting-started-with-red-hat-business-automation-version-711.html</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tCiWOcx72HQ/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" /><author><name>Wojciech Trocki</name></author><id>10f9bcc0-0deb-43fc-b6ef-989de0c9520a</id><updated>2021-06-22T07:00:00Z</updated><published>2021-06-22T07:00:00Z</published><summary type="html">&lt;p&gt;In this part of our ongoing introduction to the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture&lt;/a&gt;, we dig into some of the discussions the team had when developing the GraphQL section of the reference architecture. Learn about the principles we considered and gain additional insight into how we developed the current recommendations for using GraphQL in your Node.js applications.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Part 1&lt;/a&gt;: Overview of the Node.js reference architecture&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Part 2&lt;/a&gt;: Logging in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Part 3&lt;/a&gt;: Code consistency in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Part 4&lt;/strong&gt;: GraphQL inÂ Node.js&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;GraphQL in the Node.js ecosystem&lt;/h2&gt; &lt;p&gt;GraphQL is a &lt;a href="https://graphql.org/learn/" target="_blank"&gt;query language specification&lt;/a&gt; that includes specific semantics for interaction between the client and server. Implementing a GraphQL server and client typically requires more effort than building REST applications, due to the extensive nature of the language and additional requirements for client-side and server-side developers. To start, let's consider a few of the elements of developing a Node.js application with GraphQL (Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png?itok=EeUSKWlI" width="600" height="643" alt=""We need to build a new app. Let's use GraphQL!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Let's use GraphQL for our new app. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Developing a GraphQL schema&lt;/h3&gt; &lt;p&gt;When building a GraphQL API, client- and server-side teams must define strong contracts in the form of a GraphQL schema. The two teams must also change the way they have been communicating and developing their software. GraphQL internally requires server-side developers to build data-handling methods, called &lt;em&gt;resolvers&lt;/em&gt;, that match the &lt;em&gt;GraphQL schema&lt;/em&gt;, which is an internal graph that both teams must build and agree on. Client-side developers typically need to use specialized clients to send GraphQL queries to the back-end server.&lt;/p&gt; &lt;h3&gt;Choosing your tools&lt;/h3&gt; &lt;p&gt;The GraphQL ecosystem consists of thousands of libraries and solutions that you can find on GitHub, at conferences, and in various forums that offer to resolve all your GraphQL problems. On top of frameworks and libraries (Figure 2) the GraphQL ecosystem offers many out-of-the-box, self-hosted, or even service-based (SaaS) CRUD engines. Create, read, update, and delete (CRUD) engines offer to minimize the amount of server-side development by providing a direct link to the database. We'll come back to this topic later.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png?itok=GHYpODOu" width="600" height="660" alt=""Should we use a service, framework, or CRUD engine?"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: What tools will we use to enable GraphQL? &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Implementing a GraphQL API&lt;/h3&gt; &lt;p&gt;When implementing a GraphQL API, we often see a number of side-effects on other elements of our back-end infrastructure. A GraphQL API is typically exposed as a single endpoint by our back end, as illustrated in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-3.png?itok=S1EFBsvN" width="600" height="397" alt="The visual shows that a GraphQL endpoint is exposed as a single endpoint." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Unlike a REST API, a GraphQL API is exposed as a single endpoint. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Adopting the GraphQL API means that we will not only need to change the API but will often have to rethink our entire infrastructure (Figure 4), from API management and security to caching, developing a federation of queries on gateways, and much more.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png?itok=xcMaHNpT" width="600" height="663" alt="How do we get the product to work with the legacy back end?" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Think through your GraphQL-based application before implementing it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Schema first or code first?&lt;/h3&gt; &lt;p&gt;There are multiple ways to develop GraphQL solutions. The two most common approaches are &lt;em&gt;schema first&lt;/em&gt;, where developers write GraphQL schema first and later build client-side queries and data resolvers on the back end, and &lt;em&gt;code first&lt;/em&gt; (also known as resolvers first), where developers write the resolvers first and then generate the GraphQL schema for them.&lt;/p&gt; &lt;p&gt;Both approaches come with advantages and disadvantages based on your specific use case.&lt;/p&gt; &lt;h2&gt;Implementing GraphQL for Node.js&lt;/h2&gt; &lt;p&gt;Making all of the decisions about how to implement GraphQL can be daunting, as illustrated by Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png?itok=mKvv77y7" width="600" height="672" alt=""Doing this on my own is really going to take time."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Implementing GraphQL for Node.js is no simple task. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Many developers become overwhelmed by the amount of work required and look for libraries or tools that offer comprehensive support instead. As we've previously mentioned, in a GraphQL ecosystem, developers often look to one of the available CRUD engines for support (Figure 6).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png?itok=enT_X-xt" width="600" height="660" alt=""Let's use a CRUD engine!"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using a CRUD engine is a tempting workaround. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Â &lt;/p&gt; &lt;p&gt;CRUD engines try to address the major shortcomings and complexity of GraphQL by offering unified and low-code data access. However, in the long run, they can fail to deliver the capabilities we want, especially integration with other services.&lt;/p&gt; &lt;p&gt;Moreover, the initial results associated with using productivity tooling are often the tip of the iceberg for what you will face when deploying your code to production (see Figure 7).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-7.png?itok=L_ANtheg" width="600" height="396" alt="Tools only address the surface issues of development." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Considerations for developing a Node.js application with GraphQL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Red Hat team members have been using GraphQL for many years, working with the community and customers to address different challenges encountered when using GraphQL, including those we've discussed in the preceding sections. Next, we'll introduce the GraphQL section of the Node.js Reference architecture, which is based on our experience as well as that of teams within IBM.&lt;/p&gt; &lt;h2&gt;GraphQL recommendations and guidance&lt;/h2&gt; &lt;p&gt;When working on the GraphQL section of the reference architecture, we discussed a number of principles and values that influenced the documented recommendations and guidance. Here, we'll offer a brief overview.&lt;/p&gt; &lt;h3&gt;Schema first development&lt;/h3&gt; &lt;p&gt;In order to support collaboration across different languages, microservices, and tools we recommend using the GraphQL schema as a form of API definition rather than generating a schema from the code. Code-first solutions typically are limited to a single language and can create compatibility issues between the front end and other useful GraphQL tools.&lt;/p&gt; &lt;h3&gt;Separate concerns&lt;/h3&gt; &lt;p&gt;When our back- and front-end codebase is minimal we can use tools to generate code, analyze our schemas, and so on. Those tools typically do not run in production but provide a number of features missing in the reference architecture. All elements should work outside your application and can be replaced if needed.&lt;/p&gt; &lt;h3&gt;Use the GraphQL reference implementation&lt;/h3&gt; &lt;p&gt;Using the GraphQL reference implementation facilitates supportability and it is vendor agnostic. GraphQL is a Linux Foundation project with a number of reference libraries maintained under its umbrella. Choosing these libraries over single vendor and product-focused open source libraries reduces the risk of providing support and maximizes the stability of our solutions over extended periods of time.&lt;/p&gt; &lt;h3&gt;Minimalism&lt;/h3&gt; &lt;p&gt;Developers often look for libraries that offer an improved API and increase productivity. In our experience, picking a high-level tool that focuses only on the essential elements needed to build a successful GraphQL API leads to the best outcome. As a result, we've chosen to include a very short list of packages and recommendations that are useful for developers.&lt;/p&gt; &lt;h3&gt;Exclude opinionated solutions&lt;/h3&gt; &lt;p&gt;The GraphQL section of the Node.js reference architecture does not include CRUD engines or tools that affect developer flexibility and introduce proprietary APIs.&lt;/p&gt; &lt;p&gt;Based on our discussion of these principles and values, along with our prior experience, we developed the recommendations and guidance captured in the reference architecture. We hope this article has given you some insight into the background and considerations the team covered in building that section. For more information, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/graphql.md" target="_blank"&gt;GraphQL section of the Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png?itok=gJm9je_w" width="600" height="662" alt="GraphQL works!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: GraphQL works! &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Whatâs next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/" target="_blank"&gt;Node.js reference architecture series&lt;/a&gt;. While you wait for the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you'll see the work weâve already done and the kinds of topics you can look forward to in the future.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://www.redhat.com/en/topics/api/what-is-graphql" target="_blank"&gt;GraphQL&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/nodejs" target="_blank"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" title="Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js"&gt;Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tCiWOcx72HQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Wojciech Trocki</dc:creator><dc:date>2021-06-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs</feedburner:origLink></entry><entry><title type="html">WildFly 24 S2I images have been released on quay.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZRmyQLw3FJE/" /><author><name>Jean-FranÃ§ois Denise</name></author><id>https://wildfly.org//news/2021/06/22/WildFly-s2i-24-Released/</id><updated>2021-06-22T00:00:00Z</updated><content type="html">WILDFLY 24 S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 24 have been released on . The S2I builder image has been upgraded with the following content: * . This feature-pack defines some Galleon layers to provision drivers and datasources for: MariaDB, Microsoft SQL Server, MySQL, Oracle DB and PostgreSQL. You can learn how to configure the drivers by reading the feature-pack . * adapter has been upgraded to the version 13.0.1. For a complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . HELM CHART FOR WILDFLY has been updated to use the WildFly S2I 24.0 images. The is a good place to start using Helm Chart with the WildFly S2I images. Enjoy!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZRmyQLw3FJE" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-FranÃ§ois Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/06/22/WildFly-s2i-24-Released/</feedburner:origLink></entry><entry><title type="html">Kamelet for streaming to Kafka!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ckePpzU5Yyg/kamelet-for-streaming-to-kafka.html" /><author><name>CHRISTINA ã® Jèé</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</id><updated>2021-06-21T14:10:00Z</updated><content type="html">You want Kafka to stream and process the data. But what comes after you set up the platform, planned the partitioning strategy, storage options, and configured the data durability? Yes! How to stream data in and out of the platform. And this is exactly what I want to discuss today.Â  THE BACKGROUND Before we go any further, letâs see what Kafka did to make itself blazing fast? Kafka is optimized for writing the stream data in binary format, that basically logs everything directly to the file system (Sequential I/O) and makes minimum effort to process what's in the data (Optimize for Zero Copy). Â Kafka is super-charged at making sure data is stored as quickly as possible, and quickly replicating for a large number of consumers. But terrible at communication, the client that pushes content needs to SPEAK Kafka.Â  Here we are, having a super fast logging and distributing platform, but dumb at connecting to other data sources. So who is going to validate the data sent in/out of the kafka topic? What if I need to transform the data content? Can I filter the content partially? You guessed it. The clients. Â We now need smart clients that do most of the content processing and speak Kafka at the same time.Â  WHAT ARE THE MOST USED CONNECT TOOLS TODAY FOR KAFKA USERS?Â  Kafka Connect is what the majority of the Kafka users are using today. It has been broken down into many parts such as connector, tasks, worker, converter, transformer and error handler. You can view the task and worker as how the data flow is executed. For a developer they will be mostly configuring the rest 4 pieces.Â  * Connector - Describes the kind of source or the sink of the data flow, translating between the client/Kafka protocol, and knowing the libraries needed.Â  * Converter - Converts the binary to the data format accepted by the client or vice versa Â  (Currently there is limited support from Confluent, they Â only do data format) And does data format validation.Â  * Transformer - Reads into the data format, can help make simple changes to individual data chunks. Normally you would do filtering, masking or any minor changes. (Â This does not support simple calculations) * Error Handler - Define a place to store problematic data (Confluent : Dead letter queues are only applicable for sink connectors.) After configuring, it then uses Task and Worker to determine how to scale and execute that pipe data in/out of Kaka. For instance, running a cluster of works to scale and allow tasks to perform parallel processing of streams.Â  Camel is anotherÂ greatÂ option! Apache Camel is aÂ GREAT alternativeÂ for connecting Kafka too. Hereâs what Camel has to offer.Â  * Connector - Camel has more than 300+ connectors, you can use it to configure as source or the sink of the data flow, translating between the 100+client/Kafka protocol. * Converter - Â Validate and transform data formats with simple configuration. * Transformer - Not only does simple message modification, it can apply integration patterns that are good for streaming processing, such as split, filter, even customization of processes. Â  * Error Handler - Dead letter queue, catching exceptions.Â  There are also many ways to run Camel. You can have it running as a standalone single process that directly streams data in/out of Kafka .Â But Kamel works EXCEPTIONALLY well on Kubernetes.Â It run as a cluster of instances, that execute in parallel to maximize the performance. It can be deployed as native image through Quarkus to increase density and efficiency. The platform OpenShift (Kubernetes) allows users to control the scaling of the instance. Since itâs on K8s, another advantage is that operation can operate these as a unify platform, along with all other microservices.Â  WHY KAMELET? Â (THIS IS THE WAY!) One of the biggest hurdles for non Camel developers is, they need to learn another framework, maybe another language (Non-Java) to be able to get Camel running. What if we can smooth the learning curve and make it simple for newcomers? We see a great number of use cases where the masking and filtering are implemented company wide. Being able to build a repository and reuse these logics will make developers work more efficiently.Â  PLUG &amp;amp; PLAYÂ  You can look at Kamelets as templates, where you can define where to consume data from and send data to, does filtering, masking, simple calculation logic. Once the template is defined, it can be made available to the teams, that simply plugs it into the platform, configure for their needs (with either Kamelet Binding or another Camel route), and boom. The underlying Camel K will do the hard work for you, compile, build, package and deploy. You have a smart running data pipeline streams into Kafka.Â  ASSEMBLE &amp;amp; REUSE In a data pipeline, sometimes, you just need that bit of extra work on the data. Instead of defining a single template for each case, you can also break it down into smaller tasks. And assemble these small tasks to perform in the pipeline for each use case. . Â Â  STREAMS &amp;amp; SERVERLESS Kamelets allows you to stream data to/from either Kafka store or Knative event channel/broker. To be able to support Knative, Kamelet can help translate messages to CloudEvents, which is the CNCF standard event format for serverless. And also apply any pre/post-processing of the content in the pipeline.Â  SCALABLE &amp;amp; FLEXIBLE Kamelet lives on Kubernetes(can also run standalone), which gives you a comprehensive set of scaling tools, readiness, liveness check and scaling configuration. They are all part of the package. It scales by adding more instances. The UI on the OpenShift Developer Console can assist you to fill in whatâs needed. And also auto discover the available source/sink for you to choose where the data pipelines start or end.Â  UNIFY FOR DEV &amp;amp; OPSÂ  In many cases, DevOps engineers are often required to develop another set of automation tools for the deployment of connectors. Kamelet can run like other applications on kubernetes, the same tools can be used to build, deploy and monitor these pipelines. The streamline DEVOPS experience can help speed up the automation setup time. Â  MARKETPLACE List of catalogues that are already available Â (Not enough?). If you just want to stream data directly, simple pick the ones you need and start streaming. And we welcome your contributions too. What to know more about Kamelet? Take a look at this video, where it talks about why using Kamelet for streaming data to Kafka with a demo.Â  Â Introduction Â What is Kamelet?Â  Â Why do you need connectors to Kafka, and what is required in each connector?Â  Â Why Kamelet?Â  Â Marketplace of Kamelet!Â  Â Using Kamelet as a Kafka userÂ  Â Building a KameletÂ  Â Running Kamelet on KubernetesÂ  Â DemoÂ  Â Red Hat OpenShift Streams in actionÂ  Â Kamelets in action&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ckePpzU5Yyg" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA ã® Jèé</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</feedburner:origLink></entry><entry><title type="html">An Infinispan .Net Core client over the Hot Rod protocol</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/W4mNLc6TGBA/infinispan-dotnet-core-client" /><author><name>Vittorio Rigamonti</name></author><id>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</id><updated>2021-06-21T12:00:00Z</updated><content type="html">Dear Infinispanners, The Infinispan team would like to share a new project weâre working on: . Our current .NET client is based on the C++ core implementation, which is a solution that has its pros and cons. It makes it easier to guarantee parity between the C++ and C# clients over the time, but has the drawback of tying clients to specific a architecture. In turn that complicates portability and distribution, making the release lifecycle for those clients more onerous and sluggish. The is a 100% C# Hot Rod client designed with the aim of being portable across systems via the .Net Core platform and easier to deploy and consume via the Nuget platform. If you are involved with the .NET Core ecosystem we hope you will find this project of interest. Entry points for the project are: * , current status and news; * , this will contain the same testsuite of the current project with the aim of making it easier to compare with the .NET core client as it matures. * , an example of project which uses the client package. * is where the package is published. Hope this will makes C# developers happy! Please let us know your thoughts, a good place for them is the page. The Infinispan Team&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/W4mNLc6TGBA" height="1" width="1" alt=""/&gt;</content><dc:creator>Vittorio Rigamonti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</feedburner:origLink></entry></feed>
